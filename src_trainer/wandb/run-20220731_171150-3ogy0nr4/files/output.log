[2022-07-31 17:11:52,402 main.py: loading data
[2022-07-31 17:11:53,057 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-07-31 17:11:53,058 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-07-31 17:12:05,057 main.py: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-07-31 17:12:05,123 main.py: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-07-31 17:12:05,123 main.py: building model
[2022-07-31 17:12:05,750 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-07-31 17:12:05,750 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-07-31 17:12:05,824 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-07-31 17:12:08,161 main.py: expType: vis, train in expCode: vis_epoch50, batch_size 256, epoch = 1, lr = 0.0001
[2022-07-31 17:12:09,026 main.py: start training model
[2022-07-31 17:12:18,222 main.py: Epoch [1/1],  epoch_train_loss: 0.5456, epoch_val_loss: 0.4873, Train_Acc: 0.8255,  Val_Acc: 0.8761, Val_F1: 0.8211, Val_Pre: 0.9224, Val_Rec: 0.7829.
[2022-07-31 17:12:18,926 main.py: start testing model
[2022-07-31 17:12:19,551 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-07-31 17:12:19,551 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-07-31 17:12:19,580 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-07-31 17:12:22,993 main.py: Classification Acc: 0.8761, F1: 0.8211, Precision: 0.9224, Recall: 0.7829, AUC-ROC: 0.9759
[2022-07-31 17:12:22,997 main.py: Classification report:
              precision    recall  f1-score   support
           0       0.85      1.00      0.92       989
           1       0.99      0.57      0.72       391
    accuracy                           0.88      1380
   macro avg       0.92      0.78      0.82      1380
weighted avg       0.89      0.88      0.86      1380
[2022-07-31 17:12:22,997 main.py: Classification confusion matrix:
[[987   2]
 [169 222]]