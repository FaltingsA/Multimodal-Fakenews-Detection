[2022-08-02 17:50:10,398 main.py]: loading data
[2022-08-02 17:50:11,090 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-08-02 17:50:11,090 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
[2022-08-02 17:50:22,128 main.py]: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-08-02 17:50:22,184 main.py]: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 17:50:22,184 main.py]: building model
[2022-08-02 17:50:22,804 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 17:50:22,805 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 17:50:22,836 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 17:50:25,094 main.py]: expType: wo_fusion, train in expCode: wo_fusion_epoch50_freeze, batch_size 256, epoch = 50, lr = 0.0001
[2022-08-02 17:50:25,936 main.py]: start training model
Checkpoint Directory exists!
[2022-08-02 17:51:34,302 main.py]: Epoch [1/50],  epoch_train_loss: 0.5344, epoch_val_loss: 0.5819, Train_Acc: 0.8375,  Val_Acc: 0.8594, Val_F1: 0.8457, Val_Pre: 0.8329, Val_Rec: 0.8988.
Checkpoint Directory exists!
[2022-08-02 17:52:44,857 main.py]: Epoch [2/50],  epoch_train_loss: 0.4144, epoch_val_loss: 0.4304, Train_Acc: 0.9199,  Val_Acc: 0.9188, Val_F1: 0.9049, Val_Pre: 0.8899, Val_Rec: 0.9271.
Checkpoint Directory exists!
[2022-08-02 17:54:09,350 main.py]: Epoch [3/50],  epoch_train_loss: 0.3893, epoch_val_loss: 0.4108, Train_Acc: 0.9359,  Val_Acc: 0.9268, Val_F1: 0.9126, Val_Pre: 0.9020, Val_Rec: 0.9257.
[2022-08-02 17:55:33,609 main.py]: Epoch [4/50],  epoch_train_loss: 0.3758, epoch_val_loss: 0.4286, Train_Acc: 0.9453,  Val_Acc: 0.9210, Val_F1: 0.9072, Val_Pre: 0.8927, Val_Rec: 0.9279.
[2022-08-02 17:56:43,112 main.py]: Epoch [5/50],  epoch_train_loss: 0.3685, epoch_val_loss: 0.4197, Train_Acc: 0.9497,  Val_Acc: 0.9167, Val_F1: 0.8993, Val_Pre: 0.8925, Val_Rec: 0.9071.
[2022-08-02 17:57:52,369 main.py]: Epoch [6/50],  epoch_train_loss: 0.3646, epoch_val_loss: 0.4174, Train_Acc: 0.9518,  Val_Acc: 0.9116, Val_F1: 0.8892, Val_Pre: 0.8966, Val_Rec: 0.8827.
[2022-08-02 17:59:01,533 main.py]: Epoch [7/50],  epoch_train_loss: 0.3592, epoch_val_loss: 0.4195, Train_Acc: 0.9567,  Val_Acc: 0.9000, Val_F1: 0.8724, Val_Pre: 0.8882, Val_Rec: 0.8599.
[2022-08-02 18:00:10,661 main.py]: Epoch [8/50],  epoch_train_loss: 0.3549, epoch_val_loss: 0.4230, Train_Acc: 0.9615,  Val_Acc: 0.8986, Val_F1: 0.8685, Val_Pre: 0.8927, Val_Rec: 0.8511.
[2022-08-02 18:01:19,990 main.py]: Epoch [9/50],  epoch_train_loss: 0.3522, epoch_val_loss: 0.4399, Train_Acc: 0.9653,  Val_Acc: 0.8899, Val_F1: 0.8577, Val_Pre: 0.8793, Val_Rec: 0.8420.
[2022-08-02 18:02:28,884 main.py]: Epoch [10/50],  epoch_train_loss: 0.3492, epoch_val_loss: 0.4545, Train_Acc: 0.9686,  Val_Acc: 0.8478, Val_F1: 0.7811, Val_Pre: 0.8682, Val_Rec: 0.7492.
[2022-08-02 18:03:38,298 main.py]: Epoch [11/50],  epoch_train_loss: 0.3455, epoch_val_loss: 0.4668, Train_Acc: 0.9711,  Val_Acc: 0.8391, Val_F1: 0.7675, Val_Pre: 0.8552, Val_Rec: 0.7370.
[2022-08-02 18:04:47,493 main.py]: Epoch [12/50],  epoch_train_loss: 0.3431, epoch_val_loss: 0.4502, Train_Acc: 0.9731,  Val_Acc: 0.8688, Val_F1: 0.8273, Val_Pre: 0.8574, Val_Rec: 0.8080.
[2022-08-02 18:05:57,100 main.py]: Epoch [13/50],  epoch_train_loss: 0.3417, epoch_val_loss: 0.4565, Train_Acc: 0.9741,  Val_Acc: 0.8674, Val_F1: 0.8209, Val_Pre: 0.8660, Val_Rec: 0.7961.
[2022-08-02 18:07:06,275 main.py]: Epoch [14/50],  epoch_train_loss: 0.3405, epoch_val_loss: 0.4759, Train_Acc: 0.9742,  Val_Acc: 0.8428, Val_F1: 0.7805, Val_Pre: 0.8427, Val_Rec: 0.7534.
[2022-08-02 18:08:15,708 main.py]: Epoch [15/50],  epoch_train_loss: 0.3404, epoch_val_loss: 0.4378, Train_Acc: 0.9754,  Val_Acc: 0.8775, Val_F1: 0.8308, Val_Pre: 0.8939, Val_Rec: 0.8001.
[2022-08-02 18:09:24,586 main.py]: Epoch [16/50],  epoch_train_loss: 0.3384, epoch_val_loss: 0.4326, Train_Acc: 0.9766,  Val_Acc: 0.8819, Val_F1: 0.8447, Val_Pre: 0.8752, Val_Rec: 0.8248.
[2022-08-02 18:10:33,806 main.py]: Epoch [17/50],  epoch_train_loss: 0.3354, epoch_val_loss: 0.4705, Train_Acc: 0.9800,  Val_Acc: 0.8326, Val_F1: 0.7525, Val_Pre: 0.8572, Val_Rec: 0.7216.
[2022-08-02 18:11:43,425 main.py]: Epoch [18/50],  epoch_train_loss: 0.3346, epoch_val_loss: 0.4849, Train_Acc: 0.9806,  Val_Acc: 0.8232, Val_F1: 0.7433, Val_Pre: 0.8282, Val_Rec: 0.7158.
[2022-08-02 18:12:52,560 main.py]: Epoch [19/50],  epoch_train_loss: 0.3338, epoch_val_loss: 0.4831, Train_Acc: 0.9807,  Val_Acc: 0.8254, Val_F1: 0.7467, Val_Pre: 0.8317, Val_Rec: 0.7189.
[2022-08-02 18:14:01,765 main.py]: Epoch [20/50],  epoch_train_loss: 0.3334, epoch_val_loss: 0.4572, Train_Acc: 0.9814,  Val_Acc: 0.8457, Val_F1: 0.7756, Val_Pre: 0.8711, Val_Rec: 0.7431.
[2022-08-02 18:15:10,669 main.py]: Epoch [21/50],  epoch_train_loss: 0.3322, epoch_val_loss: 0.4929, Train_Acc: 0.9826,  Val_Acc: 0.8145, Val_F1: 0.7199, Val_Pre: 0.8341, Val_Rec: 0.6927.
[2022-08-02 18:16:19,880 main.py]: Epoch [22/50],  epoch_train_loss: 0.3322, epoch_val_loss: 0.4489, Train_Acc: 0.9820,  Val_Acc: 0.8616, Val_F1: 0.8104, Val_Pre: 0.8639, Val_Rec: 0.7836.
[2022-08-02 18:17:28,712 main.py]: Epoch [23/50],  epoch_train_loss: 0.3320, epoch_val_loss: 0.4505, Train_Acc: 0.9824,  Val_Acc: 0.8449, Val_F1: 0.7738, Val_Pre: 0.8721, Val_Rec: 0.7410.
[2022-08-02 18:18:38,217 main.py]: Epoch [24/50],  epoch_train_loss: 0.3307, epoch_val_loss: 0.4572, Train_Acc: 0.9837,  Val_Acc: 0.8500, Val_F1: 0.7814, Val_Pre: 0.8809, Val_Rec: 0.7477.
[2022-08-02 18:19:47,267 main.py]: Epoch [25/50],  epoch_train_loss: 0.3305, epoch_val_loss: 0.4584, Train_Acc: 0.9841,  Val_Acc: 0.8543, Val_F1: 0.7988, Val_Pre: 0.8564, Val_Rec: 0.7716.
[2022-08-02 18:20:56,865 main.py]: Epoch [26/50],  epoch_train_loss: 0.3319, epoch_val_loss: 0.4794, Train_Acc: 0.9821,  Val_Acc: 0.7978, Val_F1: 0.6784, Val_Pre: 0.8333, Val_Rec: 0.6571.
[2022-08-02 18:22:05,755 main.py]: Epoch [27/50],  epoch_train_loss: 0.3320, epoch_val_loss: 0.4654, Train_Acc: 0.9825,  Val_Acc: 0.8348, Val_F1: 0.7505, Val_Pre: 0.8774, Val_Rec: 0.7177.
[2022-08-02 18:23:14,626 main.py]: Epoch [28/50],  epoch_train_loss: 0.3305, epoch_val_loss: 0.4694, Train_Acc: 0.9839,  Val_Acc: 0.8399, Val_F1: 0.7591, Val_Pre: 0.8850, Val_Rec: 0.7251.
[2022-08-02 18:24:23,632 main.py]: Epoch [29/50],  epoch_train_loss: 0.3293, epoch_val_loss: 0.4722, Train_Acc: 0.9844,  Val_Acc: 0.8362, Val_F1: 0.7533, Val_Pre: 0.8784, Val_Rec: 0.7203.
[2022-08-02 18:25:33,192 main.py]: Epoch [30/50],  epoch_train_loss: 0.3301, epoch_val_loss: 0.4489, Train_Acc: 0.9837,  Val_Acc: 0.8551, Val_F1: 0.7896, Val_Pre: 0.8879, Val_Rec: 0.7551.
[2022-08-02 18:26:42,767 main.py]: Epoch [31/50],  epoch_train_loss: 0.3294, epoch_val_loss: 0.4541, Train_Acc: 0.9844,  Val_Acc: 0.8522, Val_F1: 0.7897, Val_Pre: 0.8690, Val_Rec: 0.7585.
[2022-08-02 18:27:52,216 main.py]: Epoch [32/50],  epoch_train_loss: 0.3296, epoch_val_loss: 0.4582, Train_Acc: 0.9845,  Val_Acc: 0.8420, Val_F1: 0.7743, Val_Pre: 0.8537, Val_Rec: 0.7444.
[2022-08-02 18:29:01,610 main.py]: Epoch [33/50],  epoch_train_loss: 0.3298, epoch_val_loss: 0.5117, Train_Acc: 0.9847,  Val_Acc: 0.7725, Val_F1: 0.6129, Val_Pre: 0.8190, Val_Rec: 0.6085.
[2022-08-02 18:30:11,062 main.py]: Epoch [34/50],  epoch_train_loss: 0.3280, epoch_val_loss: 0.4573, Train_Acc: 0.9859,  Val_Acc: 0.8478, Val_F1: 0.7826, Val_Pre: 0.8640, Val_Rec: 0.7516.
[2022-08-02 18:31:20,124 main.py]: Epoch [35/50],  epoch_train_loss: 0.3276, epoch_val_loss: 0.4899, Train_Acc: 0.9863,  Val_Acc: 0.8072, Val_F1: 0.6963, Val_Pre: 0.8492, Val_Rec: 0.6714.
[2022-08-02 18:32:29,343 main.py]: Epoch [36/50],  epoch_train_loss: 0.3292, epoch_val_loss: 0.4877, Train_Acc: 0.9859,  Val_Acc: 0.8022, Val_F1: 0.6780, Val_Pre: 0.8669, Val_Rec: 0.6563.
[2022-08-02 18:33:38,620 main.py]: Epoch [37/50],  epoch_train_loss: 0.3292, epoch_val_loss: 0.4780, Train_Acc: 0.9854,  Val_Acc: 0.8196, Val_F1: 0.7185, Val_Pre: 0.8710, Val_Rec: 0.6893.
[2022-08-02 18:34:47,799 main.py]: Epoch [38/50],  epoch_train_loss: 0.3278, epoch_val_loss: 0.4636, Train_Acc: 0.9859,  Val_Acc: 0.8428, Val_F1: 0.7786, Val_Pre: 0.8470, Val_Rec: 0.7503.
[2022-08-02 18:35:56,792 main.py]: Epoch [39/50],  epoch_train_loss: 0.3276, epoch_val_loss: 0.4670, Train_Acc: 0.9857,  Val_Acc: 0.8500, Val_F1: 0.7924, Val_Pre: 0.8504, Val_Rec: 0.7654.
[2022-08-02 18:37:06,276 main.py]: Epoch [40/50],  epoch_train_loss: 0.3273, epoch_val_loss: 0.4748, Train_Acc: 0.9863,  Val_Acc: 0.8377, Val_F1: 0.7717, Val_Pre: 0.8378, Val_Rec: 0.7445.
[2022-08-02 18:38:15,361 main.py]: Epoch [41/50],  epoch_train_loss: 0.3271, epoch_val_loss: 0.4700, Train_Acc: 0.9866,  Val_Acc: 0.8391, Val_F1: 0.7737, Val_Pre: 0.8403, Val_Rec: 0.7463.
[2022-08-02 18:39:24,653 main.py]: Epoch [42/50],  epoch_train_loss: 0.3281, epoch_val_loss: 0.4493, Train_Acc: 0.9858,  Val_Acc: 0.8696, Val_F1: 0.8264, Val_Pre: 0.8627, Val_Rec: 0.8046.
[2022-08-02 18:40:33,417 main.py]: Epoch [43/50],  epoch_train_loss: 0.3278, epoch_val_loss: 0.4780, Train_Acc: 0.9861,  Val_Acc: 0.8355, Val_F1: 0.7652, Val_Pre: 0.8414, Val_Rec: 0.7368.
[2022-08-02 18:41:42,753 main.py]: Epoch [44/50],  epoch_train_loss: 0.3268, epoch_val_loss: 0.4933, Train_Acc: 0.9868,  Val_Acc: 0.8232, Val_F1: 0.7396, Val_Pre: 0.8363, Val_Rec: 0.7112.
[2022-08-02 18:42:51,962 main.py]: Epoch [45/50],  epoch_train_loss: 0.3265, epoch_val_loss: 0.5026, Train_Acc: 0.9870,  Val_Acc: 0.8022, Val_F1: 0.6978, Val_Pre: 0.8146, Val_Rec: 0.6741.
[2022-08-02 18:44:01,564 main.py]: Epoch [46/50],  epoch_train_loss: 0.3260, epoch_val_loss: 0.4697, Train_Acc: 0.9876,  Val_Acc: 0.8370, Val_F1: 0.7699, Val_Pre: 0.8382, Val_Rec: 0.7424.
[2022-08-02 18:45:10,704 main.py]: Epoch [47/50],  epoch_train_loss: 0.3265, epoch_val_loss: 0.4584, Train_Acc: 0.9868,  Val_Acc: 0.8667, Val_F1: 0.8219, Val_Pre: 0.8601, Val_Rec: 0.7995.
[2022-08-02 18:46:19,993 main.py]: Epoch [48/50],  epoch_train_loss: 0.3259, epoch_val_loss: 0.4424, Train_Acc: 0.9875,  Val_Acc: 0.8739, Val_F1: 0.8312, Val_Pre: 0.8717, Val_Rec: 0.8076.
[2022-08-02 18:47:28,828 main.py]: Epoch [49/50],  epoch_train_loss: 0.3260, epoch_val_loss: 0.4474, Train_Acc: 0.9874,  Val_Acc: 0.8674, Val_F1: 0.8213, Val_Pre: 0.8651, Val_Rec: 0.7969.
[2022-08-02 18:48:37,788 main.py]: Epoch [50/50],  epoch_train_loss: 0.3259, epoch_val_loss: 0.4573, Train_Acc: 0.9875,  Val_Acc: 0.8572, Val_F1: 0.8024, Val_Pre: 0.8622, Val_Rec: 0.7744.
[2022-08-02 18:48:37,788 main.py]: start testing model
[2022-08-02 18:48:38,388 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 18:48:38,388 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 18:48:38,492 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-02 18:48:45,976 main.py]: showing the cache model metrics
[2022-08-02 18:48:45,977 main.py]: Classification Acc: 0.9268, F1: 0.9126, Precision: 0.9020, Recall: 0.9257, AUC-ROC: 0.9843
[2022-08-02 18:48:45,980 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.97      0.93      0.95       989
           1       0.84      0.92      0.88       391
    accuracy                           0.93      1380
   macro avg       0.90      0.93      0.91      1380
weighted avg       0.93      0.93      0.93      1380
[2022-08-02 18:48:45,980 main.py]: Classification confusion matrix:
[[918  71]
 [ 30 361]]