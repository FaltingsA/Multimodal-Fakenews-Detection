[2022-08-02 17:07:22,975 main.py]: loading data
[2022-08-02 17:07:23,651 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-08-02 17:07:23,651 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 17:07:34,541 main.py]: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-08-02 17:07:34,600 main.py]: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 17:07:34,600 main.py]: building model
[2022-08-02 17:07:35,213 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 17:07:35,214 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 17:07:35,243 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-02 17:07:37,501 main.py]: expType: text, train in expCode: text_epoch50_freeze, batch_size 256, epoch = 50, lr = 0.0001
[2022-08-02 17:07:38,340 main.py]: start training model
Checkpoint Directory exists!
[2022-08-02 17:08:27,255 main.py]: Epoch [1/50],  epoch_train_loss: 0.6936, epoch_val_loss: 0.6662, Train_Acc: 0.5090,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:09:18,619 main.py]: Epoch [2/50],  epoch_train_loss: 0.6918, epoch_val_loss: 0.6724, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:10:09,220 main.py]: Epoch [3/50],  epoch_train_loss: 0.6916, epoch_val_loss: 0.6736, Train_Acc: 0.5295,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:10:59,965 main.py]: Epoch [4/50],  epoch_train_loss: 0.6917, epoch_val_loss: 0.6687, Train_Acc: 0.5300,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:11:50,991 main.py]: Epoch [5/50],  epoch_train_loss: 0.6918, epoch_val_loss: 0.6660, Train_Acc: 0.5299,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:12:41,767 main.py]: Epoch [6/50],  epoch_train_loss: 0.6917, epoch_val_loss: 0.6683, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:13:32,463 main.py]: Epoch [7/50],  epoch_train_loss: 0.6914, epoch_val_loss: 0.6783, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:14:23,030 main.py]: Epoch [8/50],  epoch_train_loss: 0.6913, epoch_val_loss: 0.6685, Train_Acc: 0.5296,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:15:13,989 main.py]: Epoch [9/50],  epoch_train_loss: 0.6909, epoch_val_loss: 0.6693, Train_Acc: 0.5300,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:16:04,399 main.py]: Epoch [10/50],  epoch_train_loss: 0.6909, epoch_val_loss: 0.6663, Train_Acc: 0.5295,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:16:55,294 main.py]: Epoch [11/50],  epoch_train_loss: 0.6913, epoch_val_loss: 0.6622, Train_Acc: 0.5303,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:17:45,685 main.py]: Epoch [12/50],  epoch_train_loss: 0.6906, epoch_val_loss: 0.6731, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:18:36,364 main.py]: Epoch [13/50],  epoch_train_loss: 0.6910, epoch_val_loss: 0.6758, Train_Acc: 0.5299,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:19:26,596 main.py]: Epoch [14/50],  epoch_train_loss: 0.6907, epoch_val_loss: 0.6727, Train_Acc: 0.5296,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:20:16,932 main.py]: Epoch [15/50],  epoch_train_loss: 0.6899, epoch_val_loss: 0.6770, Train_Acc: 0.5320,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:21:07,184 main.py]: Epoch [16/50],  epoch_train_loss: 0.6899, epoch_val_loss: 0.6699, Train_Acc: 0.5304,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:21:57,630 main.py]: Epoch [17/50],  epoch_train_loss: 0.6895, epoch_val_loss: 0.6804, Train_Acc: 0.5293,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:22:47,828 main.py]: Epoch [18/50],  epoch_train_loss: 0.6893, epoch_val_loss: 0.6757, Train_Acc: 0.5308,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:23:38,155 main.py]: Epoch [19/50],  epoch_train_loss: 0.6894, epoch_val_loss: 0.6743, Train_Acc: 0.5304,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:24:28,466 main.py]: Epoch [20/50],  epoch_train_loss: 0.6889, epoch_val_loss: 0.6654, Train_Acc: 0.5313,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:25:19,034 main.py]: Epoch [21/50],  epoch_train_loss: 0.6888, epoch_val_loss: 0.6652, Train_Acc: 0.5303,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:26:09,633 main.py]: Epoch [22/50],  epoch_train_loss: 0.6890, epoch_val_loss: 0.6799, Train_Acc: 0.5300,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:26:59,474 main.py]: Epoch [23/50],  epoch_train_loss: 0.6884, epoch_val_loss: 0.6820, Train_Acc: 0.5317,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 17:27:49,635 main.py]: Epoch [24/50],  epoch_train_loss: 0.6878, epoch_val_loss: 0.6693, Train_Acc: 0.5310,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:28:39,940 main.py]: Epoch [25/50],  epoch_train_loss: 0.6879, epoch_val_loss: 0.6662, Train_Acc: 0.5310,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:29:30,579 main.py]: Epoch [26/50],  epoch_train_loss: 0.6876, epoch_val_loss: 0.6582, Train_Acc: 0.5358,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:30:20,849 main.py]: Epoch [27/50],  epoch_train_loss: 0.6872, epoch_val_loss: 0.6670, Train_Acc: 0.5333,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:31:11,458 main.py]: Epoch [28/50],  epoch_train_loss: 0.6873, epoch_val_loss: 0.6695, Train_Acc: 0.5310,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:32:01,964 main.py]: Epoch [29/50],  epoch_train_loss: 0.6871, epoch_val_loss: 0.6752, Train_Acc: 0.5329,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 17:32:52,366 main.py]: Epoch [30/50],  epoch_train_loss: 0.6868, epoch_val_loss: 0.6666, Train_Acc: 0.5354,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:33:42,629 main.py]: Epoch [31/50],  epoch_train_loss: 0.6862, epoch_val_loss: 0.6702, Train_Acc: 0.5361,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:34:33,175 main.py]: Epoch [32/50],  epoch_train_loss: 0.6862, epoch_val_loss: 0.6775, Train_Acc: 0.5340,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 17:35:23,413 main.py]: Epoch [33/50],  epoch_train_loss: 0.6854, epoch_val_loss: 0.6693, Train_Acc: 0.5368,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:36:14,074 main.py]: Epoch [34/50],  epoch_train_loss: 0.6857, epoch_val_loss: 0.6735, Train_Acc: 0.5364,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 17:37:04,384 main.py]: Epoch [35/50],  epoch_train_loss: 0.6854, epoch_val_loss: 0.6780, Train_Acc: 0.5434,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 17:37:54,698 main.py]: Epoch [36/50],  epoch_train_loss: 0.6856, epoch_val_loss: 0.6917, Train_Acc: 0.5482,  Val_Acc: 0.3739, Val_F1: 0.3406, Val_Pre: 0.3704, Val_Rec: 0.3405.
[2022-08-02 17:38:45,080 main.py]: Epoch [37/50],  epoch_train_loss: 0.6848, epoch_val_loss: 0.6730, Train_Acc: 0.5392,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
Checkpoint Directory exists!
[2022-08-02 17:39:35,326 main.py]: Epoch [38/50],  epoch_train_loss: 0.6846, epoch_val_loss: 0.6840, Train_Acc: 0.5478,  Val_Acc: 0.7275, Val_F1: 0.4741, Val_Pre: 0.7346, Val_Rec: 0.5254.
[2022-08-02 17:40:38,087 main.py]: Epoch [39/50],  epoch_train_loss: 0.6844, epoch_val_loss: 0.6923, Train_Acc: 0.5430,  Val_Acc: 0.3739, Val_F1: 0.3486, Val_Pre: 0.3822, Val_Rec: 0.3552.
[2022-08-02 17:41:28,398 main.py]: Epoch [40/50],  epoch_train_loss: 0.6836, epoch_val_loss: 0.6582, Train_Acc: 0.5510,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 17:42:18,931 main.py]: Epoch [41/50],  epoch_train_loss: 0.6832, epoch_val_loss: 0.6808, Train_Acc: 0.5470,  Val_Acc: 0.7225, Val_F1: 0.4458, Val_Pre: 0.7538, Val_Rec: 0.5125.
[2022-08-02 17:43:09,295 main.py]: Epoch [42/50],  epoch_train_loss: 0.6835, epoch_val_loss: 0.6812, Train_Acc: 0.5570,  Val_Acc: 0.7254, Val_F1: 0.4561, Val_Pre: 0.7786, Val_Rec: 0.5177.
[2022-08-02 17:43:59,945 main.py]: Epoch [43/50],  epoch_train_loss: 0.6833, epoch_val_loss: 0.6746, Train_Acc: 0.5450,  Val_Acc: 0.7167, Val_F1: 0.4200, Val_Pre: 0.6085, Val_Rec: 0.5008.
[2022-08-02 17:44:50,219 main.py]: Epoch [44/50],  epoch_train_loss: 0.6829, epoch_val_loss: 0.6771, Train_Acc: 0.5551,  Val_Acc: 0.7167, Val_F1: 0.4248, Val_Pre: 0.6088, Val_Rec: 0.5023.
[2022-08-02 17:45:40,787 main.py]: Epoch [45/50],  epoch_train_loss: 0.6819, epoch_val_loss: 0.6727, Train_Acc: 0.5504,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 17:46:31,242 main.py]: Epoch [46/50],  epoch_train_loss: 0.6820, epoch_val_loss: 0.6851, Train_Acc: 0.5564,  Val_Acc: 0.6188, Val_F1: 0.4638, Val_Pre: 0.4655, Val_Rec: 0.4750.
[2022-08-02 17:47:22,012 main.py]: Epoch [47/50],  epoch_train_loss: 0.6816, epoch_val_loss: 0.6742, Train_Acc: 0.5511,  Val_Acc: 0.7174, Val_F1: 0.4251, Val_Pre: 0.6589, Val_Rec: 0.5028.
[2022-08-02 17:48:12,403 main.py]: Epoch [48/50],  epoch_train_loss: 0.6812, epoch_val_loss: 0.6572, Train_Acc: 0.5611,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 17:49:02,559 main.py]: Epoch [49/50],  epoch_train_loss: 0.6821, epoch_val_loss: 0.6896, Train_Acc: 0.5532,  Val_Acc: 0.3891, Val_F1: 0.3528, Val_Pre: 0.3801, Val_Rec: 0.3527.
[2022-08-02 17:49:52,946 main.py]: Epoch [50/50],  epoch_train_loss: 0.6804, epoch_val_loss: 0.6770, Train_Acc: 0.5618,  Val_Acc: 0.7254, Val_F1: 0.4626, Val_Pre: 0.7375, Val_Rec: 0.5200.
[2022-08-02 17:49:52,946 main.py]: start testing model
[2022-08-02 17:49:53,532 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 17:49:53,533 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 17:49:53,615 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-02 17:50:00,593 main.py]: showing the cache model metrics
[2022-08-02 17:50:00,593 main.py]: Classification Acc: 0.7275, F1: 0.4741, Precision: 0.7346, Recall: 0.5254, AUC-ROC: 0.4081
[2022-08-02 17:50:00,596 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.73      0.99      0.84       989
           1       0.74      0.06      0.11       391
    accuracy                           0.73      1380
   macro avg       0.73      0.53      0.47      1380
weighted avg       0.73      0.73      0.63      1380
[2022-08-02 17:50:00,596 main.py]: Classification confusion matrix:
[[981   8]
 [368  23]]