[2022-08-02 21:33:38,657 main.py]: loading data
[2022-08-02 21:33:38,658 tokenization_utils.py]: Model name 'hfl/chinese-roberta-wwm-ext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'hfl/chinese-roberta-wwm-ext' is a path, a model identifier, or url to a directory containing tokenizer files.
[2022-08-02 21:33:39,996 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/vocab.txt from cache at /home/v-zuangao/.cache/torch/transformers/5593eb652e3fb9a17042385245a61389ce6f0c8a25e167519477d7efbdf2459a.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[2022-08-02 21:33:39,996 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/added_tokens.json from cache at /home/v-zuangao/.cache/torch/transformers/23740a16768d945f44a24590dc8f5e572773b1b2868c5e58f7ff4fae2a721c49.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
[2022-08-02 21:33:39,996 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/special_tokens_map.json from cache at /home/v-zuangao/.cache/torch/transformers/6f13f9fe28f96dd7be36b84708332115ef90b3b310918502c13a8f719a225de2.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
[2022-08-02 21:33:39,996 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/tokenizer_config.json from cache at /home/v-zuangao/.cache/torch/transformers/5bb5761fdb6c8f42bf7705c27c48cffd8b40afa8278fa035bc81bf288f108af9.1ade4e0ac224a06d83f2cb9821a6656b6b59974d6552e8c728f2657e4ba445d9
Building prefix dict from the default dictionary ...
[2022-08-02 21:35:43,220 __init__.py]: Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
[2022-08-02 21:35:43,220 __init__.py]: Loading model from cache /tmp/jieba.cache
Loading model cost 0.481 seconds.
[2022-08-02 21:35:43,701 __init__.py]: Loading model cost 0.481 seconds.
Prefix dict has been built successfully.
[2022-08-02 21:35:43,702 __init__.py]: Prefix dict has been built successfully.
TEXT: 4766, Image: 4766, label: 4766, Mask: 4766
TEXT: 1331, Image: 1331, label: 1331, Mask: 1331
[2022-08-02 21:38:04,403 main.py]: Train data_length => TEXT: 4766, Image: 4766, label: 4766, Mask: 4766
[2022-08-02 21:38:04,460 main.py]: Test data_length => TEXT: 1331, Image: 1331, label: 1331, Mask: 1331
[2022-08-02 21:38:04,461 main.py]: building model
[2022-08-02 21:38:05,056 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/config.json from cache at /home/v-zuangao/.cache/torch/transformers/3227a4c62d38cb0f891e7ccb8fc0b7b7dbd873889b3688c2e95834fb6f6109be.573c56a39f56f50b7bc6e9803d92871c74059419e37e87c5b14d49b1546b0703
[2022-08-02 21:38:05,056 configuration_utils.py]: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
[2022-08-02 21:38:05,187 modeling_utils.py]: loading weights file https://cdn.huggingface.co/hfl/chinese-roberta-wwm-ext/pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/47d2326d47246cef3121d70d592c0391a4ed594b04ce3dea8bd47edd37e20370.6ac27309c356295f0e005c6029fce503ec6a32853911ebf79f8bddd8dd10edad
[2022-08-02 21:38:06,437 main.py]: expType: vis, train in expCode: vis_epoch50_freeze, batch_size 256, epoch = 50, lr = 0.0001
[2022-08-02 21:38:07,262 main.py]: start training model
Checkpoint Directory exists!
[2022-08-02 21:38:11,644 main.py]: Epoch [1/50],  epoch_train_loss: 0.6841, epoch_val_loss: 0.7142, Train_Acc: 0.5587,  Val_Acc: 0.4884, Val_F1: 0.4667, Val_Pre: 0.4807, Val_Rec: 0.4834.
Checkpoint Directory exists!
[2022-08-02 21:38:15,732 main.py]: Epoch [2/50],  epoch_train_loss: 0.6625, epoch_val_loss: 0.7068, Train_Acc: 0.6032,  Val_Acc: 0.5079, Val_F1: 0.4985, Val_Pre: 0.5049, Val_Rec: 0.5046.
Checkpoint Directory exists!
[2022-08-02 21:38:29,889 main.py]: Epoch [3/50],  epoch_train_loss: 0.6531, epoch_val_loss: 0.6939, Train_Acc: 0.6183,  Val_Acc: 0.5274, Val_F1: 0.5215, Val_Pre: 0.5258, Val_Rec: 0.5249.
Checkpoint Directory exists!
[2022-08-02 21:38:46,620 main.py]: Epoch [4/50],  epoch_train_loss: 0.6452, epoch_val_loss: 0.6856, Train_Acc: 0.6303,  Val_Acc: 0.5485, Val_F1: 0.5413, Val_Pre: 0.5479, Val_Rec: 0.5456.
Checkpoint Directory exists!
[2022-08-02 21:39:00,920 main.py]: Epoch [5/50],  epoch_train_loss: 0.6384, epoch_val_loss: 0.6828, Train_Acc: 0.6450,  Val_Acc: 0.5567, Val_F1: 0.5538, Val_Pre: 0.5560, Val_Rec: 0.5550.
Checkpoint Directory exists!
[2022-08-02 21:39:18,069 main.py]: Epoch [6/50],  epoch_train_loss: 0.6322, epoch_val_loss: 0.6804, Train_Acc: 0.6574,  Val_Acc: 0.5650, Val_F1: 0.5605, Val_Pre: 0.5647, Val_Rec: 0.5627.
Checkpoint Directory exists!
[2022-08-02 21:39:32,369 main.py]: Epoch [7/50],  epoch_train_loss: 0.6279, epoch_val_loss: 0.6744, Train_Acc: 0.6616,  Val_Acc: 0.5800, Val_F1: 0.5772, Val_Pre: 0.5797, Val_Rec: 0.5783.
[2022-08-02 21:39:48,059 main.py]: Epoch [8/50],  epoch_train_loss: 0.6237, epoch_val_loss: 0.6732, Train_Acc: 0.6700,  Val_Acc: 0.5800, Val_F1: 0.5707, Val_Pre: 0.5824, Val_Rec: 0.5765.
Checkpoint Directory exists!
[2022-08-02 21:39:51,190 main.py]: Epoch [9/50],  epoch_train_loss: 0.6207, epoch_val_loss: 0.6748, Train_Acc: 0.6723,  Val_Acc: 0.5823, Val_F1: 0.5800, Val_Pre: 0.5819, Val_Rec: 0.5808.
Checkpoint Directory exists!
[2022-08-02 21:40:05,651 main.py]: Epoch [10/50],  epoch_train_loss: 0.6171, epoch_val_loss: 0.6683, Train_Acc: 0.6744,  Val_Acc: 0.5950, Val_F1: 0.5926, Val_Pre: 0.5949, Val_Rec: 0.5934.
[2022-08-02 21:40:19,363 main.py]: Epoch [11/50],  epoch_train_loss: 0.6140, epoch_val_loss: 0.6701, Train_Acc: 0.6840,  Val_Acc: 0.5898, Val_F1: 0.5874, Val_Pre: 0.5895, Val_Rec: 0.5882.
Checkpoint Directory exists!
[2022-08-02 21:40:22,317 main.py]: Epoch [12/50],  epoch_train_loss: 0.6101, epoch_val_loss: 0.6660, Train_Acc: 0.6874,  Val_Acc: 0.5973, Val_F1: 0.5941, Val_Pre: 0.5976, Val_Rec: 0.5954.
Checkpoint Directory exists!
[2022-08-02 21:40:40,905 main.py]: Epoch [13/50],  epoch_train_loss: 0.6074, epoch_val_loss: 0.6657, Train_Acc: 0.6956,  Val_Acc: 0.5988, Val_F1: 0.5956, Val_Pre: 0.5991, Val_Rec: 0.5969.
[2022-08-02 21:40:55,514 main.py]: Epoch [14/50],  epoch_train_loss: 0.6049, epoch_val_loss: 0.6623, Train_Acc: 0.6970,  Val_Acc: 0.5980, Val_F1: 0.5935, Val_Pre: 0.5990, Val_Rec: 0.5957.
Checkpoint Directory exists!
[2022-08-02 21:40:58,585 main.py]: Epoch [15/50],  epoch_train_loss: 0.6002, epoch_val_loss: 0.6635, Train_Acc: 0.7037,  Val_Acc: 0.6018, Val_F1: 0.6008, Val_Pre: 0.6014, Val_Rec: 0.6009.
Checkpoint Directory exists!
[2022-08-02 21:41:12,376 main.py]: Epoch [16/50],  epoch_train_loss: 0.5998, epoch_val_loss: 0.6610, Train_Acc: 0.7037,  Val_Acc: 0.6108, Val_F1: 0.6091, Val_Pre: 0.6107, Val_Rec: 0.6095.
[2022-08-02 21:41:26,266 main.py]: Epoch [17/50],  epoch_train_loss: 0.5965, epoch_val_loss: 0.6616, Train_Acc: 0.7138,  Val_Acc: 0.6033, Val_F1: 0.6022, Val_Pre: 0.6029, Val_Rec: 0.6023.
[2022-08-02 21:41:29,170 main.py]: Epoch [18/50],  epoch_train_loss: 0.5955, epoch_val_loss: 0.6590, Train_Acc: 0.7163,  Val_Acc: 0.6108, Val_F1: 0.6102, Val_Pre: 0.6104, Val_Rec: 0.6102.
Checkpoint Directory exists!
[2022-08-02 21:41:32,061 main.py]: Epoch [19/50],  epoch_train_loss: 0.5928, epoch_val_loss: 0.6577, Train_Acc: 0.7163,  Val_Acc: 0.6168, Val_F1: 0.6113, Val_Pre: 0.6194, Val_Rec: 0.6141.
[2022-08-02 21:41:45,792 main.py]: Epoch [20/50],  epoch_train_loss: 0.5895, epoch_val_loss: 0.6582, Train_Acc: 0.7207,  Val_Acc: 0.6168, Val_F1: 0.6162, Val_Pre: 0.6165, Val_Rec: 0.6162.
[2022-08-02 21:41:48,720 main.py]: Epoch [21/50],  epoch_train_loss: 0.5866, epoch_val_loss: 0.6608, Train_Acc: 0.7243,  Val_Acc: 0.6086, Val_F1: 0.6076, Val_Pre: 0.6082, Val_Rec: 0.6077.
Checkpoint Directory exists!
[2022-08-02 21:41:51,656 main.py]: Epoch [22/50],  epoch_train_loss: 0.5849, epoch_val_loss: 0.6536, Train_Acc: 0.7272,  Val_Acc: 0.6243, Val_F1: 0.6220, Val_Pre: 0.6248, Val_Rec: 0.6227.
[2022-08-02 21:42:05,762 main.py]: Epoch [23/50],  epoch_train_loss: 0.5843, epoch_val_loss: 0.6550, Train_Acc: 0.7306,  Val_Acc: 0.6236, Val_F1: 0.6232, Val_Pre: 0.6233, Val_Rec: 0.6232.
[2022-08-02 21:42:08,713 main.py]: Epoch [24/50],  epoch_train_loss: 0.5809, epoch_val_loss: 0.6567, Train_Acc: 0.7369,  Val_Acc: 0.6228, Val_F1: 0.6219, Val_Pre: 0.6225, Val_Rec: 0.6219.
[2022-08-02 21:42:11,599 main.py]: Epoch [25/50],  epoch_train_loss: 0.5778, epoch_val_loss: 0.6529, Train_Acc: 0.7386,  Val_Acc: 0.6221, Val_F1: 0.6204, Val_Pre: 0.6221, Val_Rec: 0.6208.
Checkpoint Directory exists!
[2022-08-02 21:42:14,535 main.py]: Epoch [26/50],  epoch_train_loss: 0.5765, epoch_val_loss: 0.6523, Train_Acc: 0.7379,  Val_Acc: 0.6266, Val_F1: 0.6246, Val_Pre: 0.6268, Val_Rec: 0.6251.
[2022-08-02 21:42:28,465 main.py]: Epoch [27/50],  epoch_train_loss: 0.5738, epoch_val_loss: 0.6533, Train_Acc: 0.7461,  Val_Acc: 0.6258, Val_F1: 0.6243, Val_Pre: 0.6258, Val_Rec: 0.6246.
Checkpoint Directory exists!
[2022-08-02 21:42:31,461 main.py]: Epoch [28/50],  epoch_train_loss: 0.5722, epoch_val_loss: 0.6505, Train_Acc: 0.7509,  Val_Acc: 0.6289, Val_F1: 0.6277, Val_Pre: 0.6286, Val_Rec: 0.6278.
[2022-08-02 21:42:45,649 main.py]: Epoch [29/50],  epoch_train_loss: 0.5719, epoch_val_loss: 0.6543, Train_Acc: 0.7478,  Val_Acc: 0.6258, Val_F1: 0.6253, Val_Pre: 0.6255, Val_Rec: 0.6253.
Checkpoint Directory exists!
[2022-08-02 21:42:48,693 main.py]: Epoch [30/50],  epoch_train_loss: 0.5696, epoch_val_loss: 0.6503, Train_Acc: 0.7587,  Val_Acc: 0.6296, Val_F1: 0.6277, Val_Pre: 0.6299, Val_Rec: 0.6281.
[2022-08-02 21:43:02,345 main.py]: Epoch [31/50],  epoch_train_loss: 0.5680, epoch_val_loss: 0.6509, Train_Acc: 0.7530,  Val_Acc: 0.6243, Val_F1: 0.6208, Val_Pre: 0.6257, Val_Rec: 0.6222.
[2022-08-02 21:43:05,305 main.py]: Epoch [32/50],  epoch_train_loss: 0.5662, epoch_val_loss: 0.6529, Train_Acc: 0.7593,  Val_Acc: 0.6206, Val_F1: 0.6205, Val_Pre: 0.6205, Val_Rec: 0.6205.
Checkpoint Directory exists!
[2022-08-02 21:43:08,258 main.py]: Epoch [33/50],  epoch_train_loss: 0.5634, epoch_val_loss: 0.6501, Train_Acc: 0.7631,  Val_Acc: 0.6311, Val_F1: 0.6293, Val_Pre: 0.6313, Val_Rec: 0.6297.
Checkpoint Directory exists!
[2022-08-02 21:43:22,711 main.py]: Epoch [34/50],  epoch_train_loss: 0.5633, epoch_val_loss: 0.6481, Train_Acc: 0.7644,  Val_Acc: 0.6319, Val_F1: 0.6298, Val_Pre: 0.6322, Val_Rec: 0.6303.
[2022-08-02 21:43:36,726 main.py]: Epoch [35/50],  epoch_train_loss: 0.5593, epoch_val_loss: 0.6496, Train_Acc: 0.7686,  Val_Acc: 0.6266, Val_F1: 0.6260, Val_Pre: 0.6263, Val_Rec: 0.6260.
[2022-08-02 21:43:39,678 main.py]: Epoch [36/50],  epoch_train_loss: 0.5592, epoch_val_loss: 0.6511, Train_Acc: 0.7652,  Val_Acc: 0.6266, Val_F1: 0.6246, Val_Pre: 0.6269, Val_Rec: 0.6251.
Checkpoint Directory exists!
[2022-08-02 21:43:42,630 main.py]: Epoch [37/50],  epoch_train_loss: 0.5557, epoch_val_loss: 0.6489, Train_Acc: 0.7751,  Val_Acc: 0.6356, Val_F1: 0.6336, Val_Pre: 0.6360, Val_Rec: 0.6341.
[2022-08-02 21:43:58,186 main.py]: Epoch [38/50],  epoch_train_loss: 0.5544, epoch_val_loss: 0.6509, Train_Acc: 0.7763,  Val_Acc: 0.6326, Val_F1: 0.6300, Val_Pre: 0.6334, Val_Rec: 0.6308.
[2022-08-02 21:44:01,206 main.py]: Epoch [39/50],  epoch_train_loss: 0.5534, epoch_val_loss: 0.6490, Train_Acc: 0.7782,  Val_Acc: 0.6304, Val_F1: 0.6292, Val_Pre: 0.6302, Val_Rec: 0.6293.
[2022-08-02 21:44:04,164 main.py]: Epoch [40/50],  epoch_train_loss: 0.5526, epoch_val_loss: 0.6482, Train_Acc: 0.7784,  Val_Acc: 0.6258, Val_F1: 0.6255, Val_Pre: 0.6256, Val_Rec: 0.6255.
[2022-08-02 21:44:07,123 main.py]: Epoch [41/50],  epoch_train_loss: 0.5503, epoch_val_loss: 0.6487, Train_Acc: 0.7768,  Val_Acc: 0.6334, Val_F1: 0.6317, Val_Pre: 0.6335, Val_Rec: 0.6320.
[2022-08-02 21:44:10,236 main.py]: Epoch [42/50],  epoch_train_loss: 0.5477, epoch_val_loss: 0.6474, Train_Acc: 0.7870,  Val_Acc: 0.6296, Val_F1: 0.6286, Val_Pre: 0.6293, Val_Rec: 0.6287.
[2022-08-02 21:44:13,185 main.py]: Epoch [43/50],  epoch_train_loss: 0.5469, epoch_val_loss: 0.6469, Train_Acc: 0.7828,  Val_Acc: 0.6319, Val_F1: 0.6298, Val_Pre: 0.6322, Val_Rec: 0.6303.
[2022-08-02 21:44:16,203 main.py]: Epoch [44/50],  epoch_train_loss: 0.5479, epoch_val_loss: 0.6460, Train_Acc: 0.7809,  Val_Acc: 0.6311, Val_F1: 0.6298, Val_Pre: 0.6310, Val_Rec: 0.6300.
[2022-08-02 21:44:19,202 main.py]: Epoch [45/50],  epoch_train_loss: 0.5426, epoch_val_loss: 0.6473, Train_Acc: 0.7896,  Val_Acc: 0.6319, Val_F1: 0.6309, Val_Pre: 0.6316, Val_Rec: 0.6310.
[2022-08-02 21:44:22,212 main.py]: Epoch [46/50],  epoch_train_loss: 0.5417, epoch_val_loss: 0.6501, Train_Acc: 0.7906,  Val_Acc: 0.6326, Val_F1: 0.6303, Val_Pre: 0.6332, Val_Rec: 0.6309.
[2022-08-02 21:44:25,184 main.py]: Epoch [47/50],  epoch_train_loss: 0.5413, epoch_val_loss: 0.6469, Train_Acc: 0.7931,  Val_Acc: 0.6296, Val_F1: 0.6292, Val_Pre: 0.6293, Val_Rec: 0.6292.
[2022-08-02 21:44:28,141 main.py]: Epoch [48/50],  epoch_train_loss: 0.5394, epoch_val_loss: 0.6488, Train_Acc: 0.7944,  Val_Acc: 0.6296, Val_F1: 0.6271, Val_Pre: 0.6302, Val_Rec: 0.6279.
[2022-08-02 21:44:31,082 main.py]: Epoch [49/50],  epoch_train_loss: 0.5368, epoch_val_loss: 0.6489, Train_Acc: 0.7969,  Val_Acc: 0.6304, Val_F1: 0.6300, Val_Pre: 0.6300, Val_Rec: 0.6299.
[2022-08-02 21:44:34,054 main.py]: Epoch [50/50],  epoch_train_loss: 0.5356, epoch_val_loss: 0.6538, Train_Acc: 0.7973,  Val_Acc: 0.6311, Val_F1: 0.6311, Val_Pre: 0.6319, Val_Rec: 0.6318.
[2022-08-02 21:44:34,055 main.py]: start testing model
[2022-08-02 21:44:34,665 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/config.json from cache at /home/v-zuangao/.cache/torch/transformers/3227a4c62d38cb0f891e7ccb8fc0b7b7dbd873889b3688c2e95834fb6f6109be.573c56a39f56f50b7bc6e9803d92871c74059419e37e87c5b14d49b1546b0703
[2022-08-02 21:44:34,666 configuration_utils.py]: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
[2022-08-02 21:44:34,779 modeling_utils.py]: loading weights file https://cdn.huggingface.co/hfl/chinese-roberta-wwm-ext/pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/47d2326d47246cef3121d70d592c0391a4ed594b04ce3dea8bd47edd37e20370.6ac27309c356295f0e005c6029fce503ec6a32853911ebf79f8bddd8dd10edad
[2022-08-02 21:44:37,095 main.py]: showing the cache model metrics
[2022-08-02 21:44:37,096 main.py]: Classification Acc: 0.6356, F1: 0.6336, Precision: 0.6360, Recall: 0.6341, AUC-ROC: 0.6706
[2022-08-02 21:44:37,099 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.64      0.58      0.61       648
           1       0.63      0.69      0.66       683
    accuracy                           0.64      1331
   macro avg       0.64      0.63      0.63      1331
weighted avg       0.64      0.64      0.63      1331
[2022-08-02 21:44:37,099 main.py]: Classification confusion matrix:
[[374 274]
 [211 472]]