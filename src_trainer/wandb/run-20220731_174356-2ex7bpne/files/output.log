[2022-07-31 17:43:58,602 main.py: loading data
[2022-07-31 17:43:59,275 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-07-31 17:43:59,276 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-07-31 17:44:10,528 main.py: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-07-31 17:44:10,588 main.py: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-07-31 17:44:10,588 main.py: building model
[2022-07-31 17:44:11,177 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-07-31 17:44:11,177 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-07-31 17:44:11,343 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-07-31 17:44:13,588 main.py: expType: vis, train in expCode: vis_epoch50, batch_size 256, epoch = 50, lr = 0.0001
[2022-07-31 17:44:14,380 main.py: start training model
[2022-07-31 17:44:22,805 main.py: Epoch [1/50],  epoch_train_loss: 0.5456, epoch_val_loss: 0.4873, Train_Acc: 0.8255,  Val_Acc: 0.8761, Val_F1: 0.8211, Val_Pre: 0.9224, Val_Rec: 0.7829.
[2022-07-31 17:44:30,942 main.py: Epoch [2/50],  epoch_train_loss: 0.4363, epoch_val_loss: 0.4469, Train_Acc: 0.9023,  Val_Acc: 0.8862, Val_F1: 0.8384, Val_Pre: 0.9279, Val_Rec: 0.8008.
[2022-07-31 17:44:38,884 main.py: Epoch [3/50],  epoch_train_loss: 0.4110, epoch_val_loss: 0.4460, Train_Acc: 0.9196,  Val_Acc: 0.8746, Val_F1: 0.8186, Val_Pre: 0.9217, Val_Rec: 0.7803.
[2022-07-31 17:44:46,211 main.py: Epoch [4/50],  epoch_train_loss: 0.3958, epoch_val_loss: 0.4528, Train_Acc: 0.9309,  Val_Acc: 0.7572, Val_F1: 0.5915, Val_Pre: 0.7560, Val_Rec: 0.5925.
[2022-07-31 17:44:53,532 main.py: Epoch [5/50],  epoch_train_loss: 0.3871, epoch_val_loss: 0.4604, Train_Acc: 0.9383,  Val_Acc: 0.7587, Val_F1: 0.5952, Val_Pre: 0.7588, Val_Rec: 0.5950.
[2022-07-31 17:45:00,895 main.py: Epoch [6/50],  epoch_train_loss: 0.3810, epoch_val_loss: 0.4590, Train_Acc: 0.9420,  Val_Acc: 0.7565, Val_F1: 0.5897, Val_Pre: 0.7546, Val_Rec: 0.5912.
[2022-07-31 17:45:08,340 main.py: Epoch [7/50],  epoch_train_loss: 0.3767, epoch_val_loss: 0.4625, Train_Acc: 0.9442,  Val_Acc: 0.7551, Val_F1: 0.5885, Val_Pre: 0.7476, Val_Rec: 0.5902.
[2022-07-31 17:45:15,646 main.py: Epoch [8/50],  epoch_train_loss: 0.3747, epoch_val_loss: 0.4823, Train_Acc: 0.9455,  Val_Acc: 0.7565, Val_F1: 0.5897, Val_Pre: 0.7546, Val_Rec: 0.5912.
[2022-07-31 17:45:22,937 main.py: Epoch [9/50],  epoch_train_loss: 0.3715, epoch_val_loss: 0.4886, Train_Acc: 0.9468,  Val_Acc: 0.7536, Val_F1: 0.5861, Val_Pre: 0.7427, Val_Rec: 0.5884.
[2022-07-31 17:45:30,256 main.py: Epoch [10/50],  epoch_train_loss: 0.3702, epoch_val_loss: 0.4881, Train_Acc: 0.9480,  Val_Acc: 0.7536, Val_F1: 0.5861, Val_Pre: 0.7427, Val_Rec: 0.5884.
[2022-07-31 17:45:37,600 main.py: Epoch [11/50],  epoch_train_loss: 0.3688, epoch_val_loss: 0.4919, Train_Acc: 0.9488,  Val_Acc: 0.7536, Val_F1: 0.5861, Val_Pre: 0.7427, Val_Rec: 0.5884.
[2022-07-31 17:45:44,838 main.py: Epoch [12/50],  epoch_train_loss: 0.3674, epoch_val_loss: 0.4845, Train_Acc: 0.9498,  Val_Acc: 0.7558, Val_F1: 0.5916, Val_Pre: 0.7472, Val_Rec: 0.5923.
[2022-07-31 17:45:52,327 main.py: Epoch [13/50],  epoch_train_loss: 0.3674, epoch_val_loss: 0.4871, Train_Acc: 0.9500,  Val_Acc: 0.7565, Val_F1: 0.5935, Val_Pre: 0.7486, Val_Rec: 0.5935.
[2022-07-31 17:45:59,596 main.py: Epoch [14/50],  epoch_train_loss: 0.3667, epoch_val_loss: 0.4902, Train_Acc: 0.9502,  Val_Acc: 0.7558, Val_F1: 0.5916, Val_Pre: 0.7472, Val_Rec: 0.5923.
[2022-07-31 17:46:06,764 main.py: Epoch [15/50],  epoch_train_loss: 0.3662, epoch_val_loss: 0.4782, Train_Acc: 0.9504,  Val_Acc: 0.7587, Val_F1: 0.5989, Val_Pre: 0.7528, Val_Rec: 0.5974.
[2022-07-31 17:46:14,037 main.py: Epoch [16/50],  epoch_train_loss: 0.3650, epoch_val_loss: 0.4954, Train_Acc: 0.9508,  Val_Acc: 0.7543, Val_F1: 0.5879, Val_Pre: 0.7442, Val_Rec: 0.5897.
[2022-07-31 17:46:21,368 main.py: Epoch [17/50],  epoch_train_loss: 0.3656, epoch_val_loss: 0.4949, Train_Acc: 0.9505,  Val_Acc: 0.7558, Val_F1: 0.5916, Val_Pre: 0.7472, Val_Rec: 0.5923.
[2022-07-31 17:46:28,683 main.py: Epoch [18/50],  epoch_train_loss: 0.3651, epoch_val_loss: 0.4989, Train_Acc: 0.9510,  Val_Acc: 0.7674, Val_F1: 0.6203, Val_Pre: 0.7682, Val_Rec: 0.6127.
[2022-07-31 17:46:35,983 main.py: Epoch [19/50],  epoch_train_loss: 0.3640, epoch_val_loss: 0.5027, Train_Acc: 0.9510,  Val_Acc: 0.7688, Val_F1: 0.6237, Val_Pre: 0.7706, Val_Rec: 0.6153.
[2022-07-31 17:46:43,245 main.py: Epoch [20/50],  epoch_train_loss: 0.3642, epoch_val_loss: 0.4969, Train_Acc: 0.9510,  Val_Acc: 0.7659, Val_F1: 0.6168, Val_Pre: 0.7658, Val_Rec: 0.6102.
[2022-07-31 17:46:50,491 main.py: Epoch [21/50],  epoch_train_loss: 0.3639, epoch_val_loss: 0.4874, Train_Acc: 0.9511,  Val_Acc: 0.7710, Val_F1: 0.6289, Val_Pre: 0.7740, Val_Rec: 0.6191.
[2022-07-31 17:46:57,701 main.py: Epoch [22/50],  epoch_train_loss: 0.3633, epoch_val_loss: 0.4959, Train_Acc: 0.9510,  Val_Acc: 0.7674, Val_F1: 0.6180, Val_Pre: 0.7721, Val_Rec: 0.6112.
[2022-07-31 17:47:04,868 main.py: Epoch [23/50],  epoch_train_loss: 0.3631, epoch_val_loss: 0.4915, Train_Acc: 0.9512,  Val_Acc: 0.7674, Val_F1: 0.6180, Val_Pre: 0.7721, Val_Rec: 0.6112.
[2022-07-31 17:47:12,330 main.py: Epoch [24/50],  epoch_train_loss: 0.3623, epoch_val_loss: 0.4797, Train_Acc: 0.9512,  Val_Acc: 0.7674, Val_F1: 0.6180, Val_Pre: 0.7721, Val_Rec: 0.6112.
[2022-07-31 17:47:19,658 main.py: Epoch [25/50],  epoch_train_loss: 0.3633, epoch_val_loss: 0.4774, Train_Acc: 0.9519,  Val_Acc: 0.7674, Val_F1: 0.6180, Val_Pre: 0.7721, Val_Rec: 0.6112.
[2022-07-31 17:47:26,956 main.py: Epoch [26/50],  epoch_train_loss: 0.3626, epoch_val_loss: 0.4634, Train_Acc: 0.9520,  Val_Acc: 0.7746, Val_F1: 0.6353, Val_Pre: 0.7833, Val_Rec: 0.6240.
[2022-07-31 17:47:34,305 main.py: Epoch [27/50],  epoch_train_loss: 0.3625, epoch_val_loss: 0.4687, Train_Acc: 0.9522,  Val_Acc: 0.7725, Val_F1: 0.6302, Val_Pre: 0.7801, Val_Rec: 0.6201.
[2022-07-31 17:47:41,658 main.py: Epoch [28/50],  epoch_train_loss: 0.3621, epoch_val_loss: 0.4620, Train_Acc: 0.9522,  Val_Acc: 0.7710, Val_F1: 0.6267, Val_Pre: 0.7778, Val_Rec: 0.6176.
[2022-07-31 17:47:48,952 main.py: Epoch [29/50],  epoch_train_loss: 0.3618, epoch_val_loss: 0.4686, Train_Acc: 0.9521,  Val_Acc: 0.7870, Val_F1: 0.6341, Val_Pre: 0.8854, Val_Rec: 0.6240.
[2022-07-31 17:47:56,151 main.py: Epoch [30/50],  epoch_train_loss: 0.3623, epoch_val_loss: 0.4686, Train_Acc: 0.9524,  Val_Acc: 0.7710, Val_F1: 0.6256, Val_Pre: 0.7798, Val_Rec: 0.6168.
[2022-07-31 17:48:03,409 main.py: Epoch [31/50],  epoch_train_loss: 0.3622, epoch_val_loss: 0.4503, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-07-31 17:48:10,709 main.py: Epoch [32/50],  epoch_train_loss: 0.3619, epoch_val_loss: 0.4478, Train_Acc: 0.9523,  Val_Acc: 0.7746, Val_F1: 0.6353, Val_Pre: 0.7833, Val_Rec: 0.6240.
[2022-07-31 17:48:17,975 main.py: Epoch [33/50],  epoch_train_loss: 0.3615, epoch_val_loss: 0.4532, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-07-31 17:48:25,323 main.py: Epoch [34/50],  epoch_train_loss: 0.3622, epoch_val_loss: 0.4588, Train_Acc: 0.9523,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-07-31 17:48:32,762 main.py: Epoch [35/50],  epoch_train_loss: 0.3620, epoch_val_loss: 0.4439, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-07-31 17:48:40,339 main.py: Epoch [36/50],  epoch_train_loss: 0.3611, epoch_val_loss: 0.4553, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-07-31 17:48:47,705 main.py: Epoch [37/50],  epoch_train_loss: 0.3619, epoch_val_loss: 0.4530, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-07-31 17:48:55,159 main.py: Epoch [38/50],  epoch_train_loss: 0.3614, epoch_val_loss: 0.4405, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-07-31 17:49:02,558 main.py: Epoch [39/50],  epoch_train_loss: 0.3606, epoch_val_loss: 0.4462, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-07-31 17:49:09,917 main.py: Epoch [40/50],  epoch_train_loss: 0.3618, epoch_val_loss: 0.4242, Train_Acc: 0.9524,  Val_Acc: 0.8732, Val_F1: 0.8263, Val_Pre: 0.8822, Val_Rec: 0.7979.
[2022-07-31 17:49:17,374 main.py: Epoch [41/50],  epoch_train_loss: 0.3621, epoch_val_loss: 0.4388, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-07-31 17:49:24,792 main.py: Epoch [42/50],  epoch_train_loss: 0.3627, epoch_val_loss: 0.4256, Train_Acc: 0.9524,  Val_Acc: 0.8732, Val_F1: 0.8259, Val_Pre: 0.8833, Val_Rec: 0.7971.
[2022-07-31 17:49:32,364 main.py: Epoch [43/50],  epoch_train_loss: 0.3614, epoch_val_loss: 0.4719, Train_Acc: 0.9524,  Val_Acc: 0.7645, Val_F1: 0.6097, Val_Pre: 0.7692, Val_Rec: 0.6053.
[2022-07-31 17:49:39,910 main.py: Epoch [44/50],  epoch_train_loss: 0.3607, epoch_val_loss: 0.4447, Train_Acc: 0.9525,  Val_Acc: 0.7674, Val_F1: 0.6169, Val_Pre: 0.7741, Val_Rec: 0.6104.
[2022-07-31 17:49:47,381 main.py: Epoch [45/50],  epoch_train_loss: 0.3613, epoch_val_loss: 0.4299, Train_Acc: 0.9525,  Val_Acc: 0.7870, Val_F1: 0.6341, Val_Pre: 0.8854, Val_Rec: 0.6240.
[2022-07-31 17:49:54,895 main.py: Epoch [46/50],  epoch_train_loss: 0.3603, epoch_val_loss: 0.4487, Train_Acc: 0.9524,  Val_Acc: 0.7717, Val_F1: 0.6274, Val_Pre: 0.7810, Val_Rec: 0.6181.
[2022-07-31 17:50:02,586 main.py: Epoch [47/50],  epoch_train_loss: 0.3604, epoch_val_loss: 0.4381, Train_Acc: 0.9524,  Val_Acc: 0.7717, Val_F1: 0.6274, Val_Pre: 0.7810, Val_Rec: 0.6181.
[2022-07-31 17:50:10,005 main.py: Epoch [48/50],  epoch_train_loss: 0.3606, epoch_val_loss: 0.4040, Train_Acc: 0.9525,  Val_Acc: 0.8732, Val_F1: 0.8259, Val_Pre: 0.8833, Val_Rec: 0.7971.
[2022-07-31 17:50:17,362 main.py: Epoch [49/50],  epoch_train_loss: 0.3615, epoch_val_loss: 0.4196, Train_Acc: 0.9525,  Val_Acc: 0.8732, Val_F1: 0.8259, Val_Pre: 0.8833, Val_Rec: 0.7971.
[2022-07-31 17:50:24,738 main.py: Epoch [50/50],  epoch_train_loss: 0.3611, epoch_val_loss: 0.4341, Train_Acc: 0.9523,  Val_Acc: 0.7891, Val_F1: 0.6395, Val_Pre: 0.8863, Val_Rec: 0.6279.
[2022-07-31 17:50:24,739 main.py: start testing model
[2022-07-31 17:50:25,360 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-07-31 17:50:25,361 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-07-31 17:50:25,455 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-07-31 17:50:28,737 main.py: Classification Acc: 0.8862, F1: 0.8384, Precision: 0.9279, Recall: 0.8008, AUC-ROC: 0.9665
[2022-07-31 17:50:28,741 main.py: Classification report:
              precision    recall  f1-score   support
           0       0.86      1.00      0.93       989
           1       0.99      0.60      0.75       391
    accuracy                           0.89      1380
   macro avg       0.93      0.80      0.84      1380
weighted avg       0.90      0.89      0.88      1380
[2022-07-31 17:50:28,741 main.py: Classification confusion matrix:
[[987   2]
 [155 236]]