[2022-07-31 17:50:37,928 main.py: loading data
[2022-07-31 17:50:38,592 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-07-31 17:50:38,593 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-07-31 17:50:49,868 main.py: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-07-31 17:50:49,929 main.py: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-07-31 17:50:49,929 main.py: building model
[2022-07-31 17:50:50,555 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-07-31 17:50:50,555 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-07-31 17:50:50,636 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-07-31 17:50:52,886 main.py: expType: text, train in expCode: text_epoch50, batch_size 256, epoch = 50, lr = 0.0001
[2022-07-31 17:50:53,696 main.py: start training model
[2022-07-31 17:51:42,597 main.py: Epoch [1/50],  epoch_train_loss: 0.6936, epoch_val_loss: 0.6662, Train_Acc: 0.5090,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:52:33,607 main.py: Epoch [2/50],  epoch_train_loss: 0.6918, epoch_val_loss: 0.6724, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:53:24,157 main.py: Epoch [3/50],  epoch_train_loss: 0.6916, epoch_val_loss: 0.6736, Train_Acc: 0.5295,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:54:14,825 main.py: Epoch [4/50],  epoch_train_loss: 0.6917, epoch_val_loss: 0.6687, Train_Acc: 0.5300,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:55:05,646 main.py: Epoch [5/50],  epoch_train_loss: 0.6918, epoch_val_loss: 0.6660, Train_Acc: 0.5299,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:55:56,271 main.py: Epoch [6/50],  epoch_train_loss: 0.6917, epoch_val_loss: 0.6683, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:56:46,781 main.py: Epoch [7/50],  epoch_train_loss: 0.6914, epoch_val_loss: 0.6783, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:57:37,316 main.py: Epoch [8/50],  epoch_train_loss: 0.6913, epoch_val_loss: 0.6685, Train_Acc: 0.5296,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:58:27,662 main.py: Epoch [9/50],  epoch_train_loss: 0.6909, epoch_val_loss: 0.6693, Train_Acc: 0.5300,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:59:18,012 main.py: Epoch [10/50],  epoch_train_loss: 0.6909, epoch_val_loss: 0.6663, Train_Acc: 0.5295,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:00:08,340 main.py: Epoch [11/50],  epoch_train_loss: 0.6913, epoch_val_loss: 0.6622, Train_Acc: 0.5303,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:00:58,832 main.py: Epoch [12/50],  epoch_train_loss: 0.6906, epoch_val_loss: 0.6731, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:01:49,327 main.py: Epoch [13/50],  epoch_train_loss: 0.6910, epoch_val_loss: 0.6758, Train_Acc: 0.5299,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:02:39,942 main.py: Epoch [14/50],  epoch_train_loss: 0.6907, epoch_val_loss: 0.6727, Train_Acc: 0.5296,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:03:30,365 main.py: Epoch [15/50],  epoch_train_loss: 0.6899, epoch_val_loss: 0.6770, Train_Acc: 0.5320,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:04:20,799 main.py: Epoch [16/50],  epoch_train_loss: 0.6899, epoch_val_loss: 0.6699, Train_Acc: 0.5304,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:05:11,213 main.py: Epoch [17/50],  epoch_train_loss: 0.6895, epoch_val_loss: 0.6804, Train_Acc: 0.5293,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:06:01,695 main.py: Epoch [18/50],  epoch_train_loss: 0.6893, epoch_val_loss: 0.6757, Train_Acc: 0.5308,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:06:52,433 main.py: Epoch [19/50],  epoch_train_loss: 0.6894, epoch_val_loss: 0.6743, Train_Acc: 0.5304,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:07:43,097 main.py: Epoch [20/50],  epoch_train_loss: 0.6889, epoch_val_loss: 0.6654, Train_Acc: 0.5313,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:08:33,825 main.py: Epoch [21/50],  epoch_train_loss: 0.6888, epoch_val_loss: 0.6652, Train_Acc: 0.5303,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:09:24,584 main.py: Epoch [22/50],  epoch_train_loss: 0.6890, epoch_val_loss: 0.6799, Train_Acc: 0.5300,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:10:15,293 main.py: Epoch [23/50],  epoch_train_loss: 0.6884, epoch_val_loss: 0.6820, Train_Acc: 0.5317,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-07-31 18:11:05,842 main.py: Epoch [24/50],  epoch_train_loss: 0.6878, epoch_val_loss: 0.6693, Train_Acc: 0.5310,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:11:56,445 main.py: Epoch [25/50],  epoch_train_loss: 0.6879, epoch_val_loss: 0.6662, Train_Acc: 0.5310,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:12:46,854 main.py: Epoch [26/50],  epoch_train_loss: 0.6876, epoch_val_loss: 0.6582, Train_Acc: 0.5358,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:13:37,237 main.py: Epoch [27/50],  epoch_train_loss: 0.6872, epoch_val_loss: 0.6670, Train_Acc: 0.5333,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:14:27,874 main.py: Epoch [28/50],  epoch_train_loss: 0.6873, epoch_val_loss: 0.6695, Train_Acc: 0.5310,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:15:18,596 main.py: Epoch [29/50],  epoch_train_loss: 0.6871, epoch_val_loss: 0.6752, Train_Acc: 0.5329,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-07-31 18:16:09,335 main.py: Epoch [30/50],  epoch_train_loss: 0.6868, epoch_val_loss: 0.6666, Train_Acc: 0.5354,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:17:00,036 main.py: Epoch [31/50],  epoch_train_loss: 0.6862, epoch_val_loss: 0.6702, Train_Acc: 0.5361,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:17:50,467 main.py: Epoch [32/50],  epoch_train_loss: 0.6862, epoch_val_loss: 0.6775, Train_Acc: 0.5340,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-07-31 18:18:40,830 main.py: Epoch [33/50],  epoch_train_loss: 0.6854, epoch_val_loss: 0.6693, Train_Acc: 0.5368,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:19:31,471 main.py: Epoch [34/50],  epoch_train_loss: 0.6857, epoch_val_loss: 0.6735, Train_Acc: 0.5364,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-07-31 18:20:22,240 main.py: Epoch [35/50],  epoch_train_loss: 0.6854, epoch_val_loss: 0.6780, Train_Acc: 0.5434,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-07-31 18:21:13,015 main.py: Epoch [36/50],  epoch_train_loss: 0.6856, epoch_val_loss: 0.6917, Train_Acc: 0.5482,  Val_Acc: 0.3739, Val_F1: 0.3406, Val_Pre: 0.3704, Val_Rec: 0.3405.
[2022-07-31 18:22:03,859 main.py: Epoch [37/50],  epoch_train_loss: 0.6848, epoch_val_loss: 0.6730, Train_Acc: 0.5392,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-07-31 18:22:54,341 main.py: Epoch [38/50],  epoch_train_loss: 0.6846, epoch_val_loss: 0.6840, Train_Acc: 0.5478,  Val_Acc: 0.7275, Val_F1: 0.4741, Val_Pre: 0.7346, Val_Rec: 0.5254.
[2022-07-31 18:23:45,754 main.py: Epoch [39/50],  epoch_train_loss: 0.6844, epoch_val_loss: 0.6923, Train_Acc: 0.5430,  Val_Acc: 0.3739, Val_F1: 0.3486, Val_Pre: 0.3822, Val_Rec: 0.3552.
[2022-07-31 18:24:36,721 main.py: Epoch [40/50],  epoch_train_loss: 0.6836, epoch_val_loss: 0.6582, Train_Acc: 0.5510,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 18:25:27,317 main.py: Epoch [41/50],  epoch_train_loss: 0.6832, epoch_val_loss: 0.6808, Train_Acc: 0.5470,  Val_Acc: 0.7225, Val_F1: 0.4458, Val_Pre: 0.7538, Val_Rec: 0.5125.
[2022-07-31 18:26:17,644 main.py: Epoch [42/50],  epoch_train_loss: 0.6835, epoch_val_loss: 0.6812, Train_Acc: 0.5570,  Val_Acc: 0.7254, Val_F1: 0.4561, Val_Pre: 0.7786, Val_Rec: 0.5177.
[2022-07-31 18:27:07,908 main.py: Epoch [43/50],  epoch_train_loss: 0.6833, epoch_val_loss: 0.6746, Train_Acc: 0.5450,  Val_Acc: 0.7167, Val_F1: 0.4200, Val_Pre: 0.6085, Val_Rec: 0.5008.
[2022-07-31 18:27:58,235 main.py: Epoch [44/50],  epoch_train_loss: 0.6829, epoch_val_loss: 0.6771, Train_Acc: 0.5551,  Val_Acc: 0.7167, Val_F1: 0.4248, Val_Pre: 0.6088, Val_Rec: 0.5023.
[2022-07-31 18:28:48,658 main.py: Epoch [45/50],  epoch_train_loss: 0.6819, epoch_val_loss: 0.6727, Train_Acc: 0.5504,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-07-31 18:29:39,296 main.py: Epoch [46/50],  epoch_train_loss: 0.6820, epoch_val_loss: 0.6851, Train_Acc: 0.5564,  Val_Acc: 0.6188, Val_F1: 0.4638, Val_Pre: 0.4655, Val_Rec: 0.4750.
[2022-07-31 18:30:30,045 main.py: Epoch [47/50],  epoch_train_loss: 0.6816, epoch_val_loss: 0.6742, Train_Acc: 0.5511,  Val_Acc: 0.7174, Val_F1: 0.4251, Val_Pre: 0.6589, Val_Rec: 0.5028.
[2022-07-31 18:31:21,071 main.py: Epoch [48/50],  epoch_train_loss: 0.6812, epoch_val_loss: 0.6572, Train_Acc: 0.5611,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-07-31 18:32:12,031 main.py: Epoch [49/50],  epoch_train_loss: 0.6821, epoch_val_loss: 0.6896, Train_Acc: 0.5532,  Val_Acc: 0.3891, Val_F1: 0.3528, Val_Pre: 0.3801, Val_Rec: 0.3527.
[2022-07-31 18:33:02,421 main.py: Epoch [50/50],  epoch_train_loss: 0.6804, epoch_val_loss: 0.6770, Train_Acc: 0.5618,  Val_Acc: 0.7254, Val_F1: 0.4626, Val_Pre: 0.7375, Val_Rec: 0.5200.
[2022-07-31 18:33:02,421 main.py: start testing model
[2022-07-31 18:33:03,114 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-07-31 18:33:03,114 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-07-31 18:33:03,224 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-07-31 18:33:10,315 main.py: Classification Acc: 0.7275, F1: 0.4741, Precision: 0.7346, Recall: 0.5254, AUC-ROC: 0.4081
[2022-07-31 18:33:10,318 main.py: Classification report:
              precision    recall  f1-score   support
           0       0.73      0.99      0.84       989
           1       0.74      0.06      0.11       391
    accuracy                           0.73      1380
   macro avg       0.73      0.53      0.47      1380
weighted avg       0.73      0.73      0.63      1380
[2022-07-31 18:33:10,319 main.py: Classification confusion matrix:
[[981   8]
 [368  23]]