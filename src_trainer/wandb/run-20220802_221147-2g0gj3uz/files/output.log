[2022-08-02 22:11:51,136 main.py]: loading data
[2022-08-02 22:11:51,137 tokenization_utils.py]: Model name 'hfl/chinese-roberta-wwm-ext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'hfl/chinese-roberta-wwm-ext' is a path, a model identifier, or url to a directory containing tokenizer files.
[2022-08-02 22:11:52,441 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/vocab.txt from cache at /home/v-zuangao/.cache/torch/transformers/5593eb652e3fb9a17042385245a61389ce6f0c8a25e167519477d7efbdf2459a.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[2022-08-02 22:11:52,441 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/added_tokens.json from cache at /home/v-zuangao/.cache/torch/transformers/23740a16768d945f44a24590dc8f5e572773b1b2868c5e58f7ff4fae2a721c49.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
[2022-08-02 22:11:52,441 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/special_tokens_map.json from cache at /home/v-zuangao/.cache/torch/transformers/6f13f9fe28f96dd7be36b84708332115ef90b3b310918502c13a8f719a225de2.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
[2022-08-02 22:11:52,441 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/tokenizer_config.json from cache at /home/v-zuangao/.cache/torch/transformers/5bb5761fdb6c8f42bf7705c27c48cffd8b40afa8278fa035bc81bf288f108af9.1ade4e0ac224a06d83f2cb9821a6656b6b59974d6552e8c728f2657e4ba445d9
Building prefix dict from the default dictionary ...
[2022-08-02 22:14:06,415 __init__.py]: Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
[2022-08-02 22:14:06,415 __init__.py]: Loading model from cache /tmp/jieba.cache
Loading model cost 0.481 seconds.
[2022-08-02 22:14:06,896 __init__.py]: Loading model cost 0.481 seconds.
Prefix dict has been built successfully.
[2022-08-02 22:14:06,898 __init__.py]: Prefix dict has been built successfully.
TEXT: 4766, Image: 4766, label: 4766, Mask: 4766
TEXT: 1331, Image: 1331, label: 1331, Mask: 1331
[2022-08-02 22:16:29,040 main.py]: Train data_length => TEXT: 4766, Image: 4766, label: 4766, Mask: 4766
[2022-08-02 22:16:29,097 main.py]: Test data_length => TEXT: 1331, Image: 1331, label: 1331, Mask: 1331
[2022-08-02 22:16:29,097 main.py]: building model
[2022-08-02 22:16:29,741 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/config.json from cache at /home/v-zuangao/.cache/torch/transformers/3227a4c62d38cb0f891e7ccb8fc0b7b7dbd873889b3688c2e95834fb6f6109be.573c56a39f56f50b7bc6e9803d92871c74059419e37e87c5b14d49b1546b0703
[2022-08-02 22:16:29,741 configuration_utils.py]: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
[2022-08-02 22:16:29,811 modeling_utils.py]: loading weights file https://cdn.huggingface.co/hfl/chinese-roberta-wwm-ext/pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/47d2326d47246cef3121d70d592c0391a4ed594b04ce3dea8bd47edd37e20370.6ac27309c356295f0e005c6029fce503ec6a32853911ebf79f8bddd8dd10edad
[2022-08-02 22:16:31,019 main.py]: expType: wo_fusion, train in expCode: wo_fusion_epoch50_freeze, batch_size 256, epoch = 50, lr = 0.0001
[2022-08-02 22:16:31,826 main.py]: start training model
Checkpoint Directory exists!
[2022-08-02 22:16:58,347 main.py]: Epoch [1/50],  epoch_train_loss: 0.6124, epoch_val_loss: 0.5239, Train_Acc: 0.7094,  Val_Acc: 0.8407, Val_F1: 0.8403, Val_Pre: 0.8415, Val_Rec: 0.8400.
Checkpoint Directory exists!
[2022-08-02 22:17:25,439 main.py]: Epoch [2/50],  epoch_train_loss: 0.4641, epoch_val_loss: 0.4422, Train_Acc: 0.8752,  Val_Acc: 0.8850, Val_F1: 0.8850, Val_Pre: 0.8853, Val_Rec: 0.8855.
Checkpoint Directory exists!
[2022-08-02 22:18:06,736 main.py]: Epoch [3/50],  epoch_train_loss: 0.4022, epoch_val_loss: 0.4290, Train_Acc: 0.9180,  Val_Acc: 0.8911, Val_F1: 0.8910, Val_Pre: 0.8932, Val_Rec: 0.8921.
[2022-08-02 22:18:46,513 main.py]: Epoch [4/50],  epoch_train_loss: 0.3739, epoch_val_loss: 0.4305, Train_Acc: 0.9427,  Val_Acc: 0.8866, Val_F1: 0.8865, Val_Pre: 0.8904, Val_Rec: 0.8879.
[2022-08-02 22:19:12,928 main.py]: Epoch [5/50],  epoch_train_loss: 0.3676, epoch_val_loss: 0.4446, Train_Acc: 0.9459,  Val_Acc: 0.8685, Val_F1: 0.8677, Val_Pre: 0.8837, Val_Rec: 0.8712.
Checkpoint Directory exists!
[2022-08-02 22:19:39,467 main.py]: Epoch [6/50],  epoch_train_loss: 0.3627, epoch_val_loss: 0.4204, Train_Acc: 0.9507,  Val_Acc: 0.8978, Val_F1: 0.8978, Val_Pre: 0.9001, Val_Rec: 0.8989.
[2022-08-02 22:20:18,823 main.py]: Epoch [7/50],  epoch_train_loss: 0.3575, epoch_val_loss: 0.4585, Train_Acc: 0.9555,  Val_Acc: 0.8610, Val_F1: 0.8599, Val_Pre: 0.8794, Val_Rec: 0.8639.
Checkpoint Directory exists!
[2022-08-02 22:20:45,462 main.py]: Epoch [8/50],  epoch_train_loss: 0.3495, epoch_val_loss: 0.4100, Train_Acc: 0.9648,  Val_Acc: 0.9016, Val_F1: 0.9016, Val_Pre: 0.9018, Val_Rec: 0.9020.
[2022-08-02 22:21:26,315 main.py]: Epoch [9/50],  epoch_train_loss: 0.3432, epoch_val_loss: 0.4149, Train_Acc: 0.9696,  Val_Acc: 0.8956, Val_F1: 0.8956, Val_Pre: 0.8956, Val_Rec: 0.8959.
[2022-08-02 22:21:53,002 main.py]: Epoch [10/50],  epoch_train_loss: 0.3414, epoch_val_loss: 0.4298, Train_Acc: 0.9715,  Val_Acc: 0.8858, Val_F1: 0.8856, Val_Pre: 0.8916, Val_Rec: 0.8875.
Checkpoint Directory exists!
[2022-08-02 22:22:19,711 main.py]: Epoch [11/50],  epoch_train_loss: 0.3327, epoch_val_loss: 0.4053, Train_Acc: 0.9811,  Val_Acc: 0.9031, Val_F1: 0.9031, Val_Pre: 0.9030, Val_Rec: 0.9033.
[2022-08-02 22:22:58,839 main.py]: Epoch [12/50],  epoch_train_loss: 0.3312, epoch_val_loss: 0.4481, Train_Acc: 0.9834,  Val_Acc: 0.8640, Val_F1: 0.8635, Val_Pre: 0.8740, Val_Rec: 0.8662.
[2022-08-02 22:23:25,546 main.py]: Epoch [13/50],  epoch_train_loss: 0.3332, epoch_val_loss: 0.4127, Train_Acc: 0.9799,  Val_Acc: 0.8866, Val_F1: 0.8865, Val_Pre: 0.8866, Val_Rec: 0.8869.
[2022-08-02 22:23:52,284 main.py]: Epoch [14/50],  epoch_train_loss: 0.3305, epoch_val_loss: 0.4899, Train_Acc: 0.9832,  Val_Acc: 0.8325, Val_F1: 0.8304, Val_Pre: 0.8573, Val_Rec: 0.8359.
[2022-08-02 22:24:19,016 main.py]: Epoch [15/50],  epoch_train_loss: 0.3281, epoch_val_loss: 0.4246, Train_Acc: 0.9855,  Val_Acc: 0.8730, Val_F1: 0.8730, Val_Pre: 0.8743, Val_Rec: 0.8739.
[2022-08-02 22:24:45,749 main.py]: Epoch [16/50],  epoch_train_loss: 0.3266, epoch_val_loss: 0.4825, Train_Acc: 0.9870,  Val_Acc: 0.8332, Val_F1: 0.8310, Val_Pre: 0.8602, Val_Rec: 0.8368.
[2022-08-02 22:25:12,476 main.py]: Epoch [17/50],  epoch_train_loss: 0.3279, epoch_val_loss: 0.3916, Train_Acc: 0.9853,  Val_Acc: 0.9121, Val_F1: 0.9120, Val_Pre: 0.9125, Val_Rec: 0.9117.
Checkpoint Directory exists!
[2022-08-02 22:25:53,707 main.py]: Epoch [18/50],  epoch_train_loss: 0.3274, epoch_val_loss: 0.4543, Train_Acc: 0.9857,  Val_Acc: 0.8595, Val_F1: 0.8589, Val_Pre: 0.8705, Val_Rec: 0.8618.
[2022-08-02 22:26:20,446 main.py]: Epoch [19/50],  epoch_train_loss: 0.3251, epoch_val_loss: 0.4214, Train_Acc: 0.9883,  Val_Acc: 0.8873, Val_F1: 0.8871, Val_Pre: 0.8928, Val_Rec: 0.8889.
[2022-08-02 22:26:47,175 main.py]: Epoch [20/50],  epoch_train_loss: 0.3237, epoch_val_loss: 0.4361, Train_Acc: 0.9901,  Val_Acc: 0.8708, Val_F1: 0.8704, Val_Pre: 0.8785, Val_Rec: 0.8727.
[2022-08-02 22:27:13,904 main.py]: Epoch [21/50],  epoch_train_loss: 0.3256, epoch_val_loss: 0.4271, Train_Acc: 0.9880,  Val_Acc: 0.8835, Val_F1: 0.8834, Val_Pre: 0.8892, Val_Rec: 0.8852.
[2022-08-02 22:27:40,625 main.py]: Epoch [22/50],  epoch_train_loss: 0.3243, epoch_val_loss: 0.3977, Train_Acc: 0.9889,  Val_Acc: 0.9068, Val_F1: 0.9067, Val_Pre: 0.9071, Val_Rec: 0.9065.
[2022-08-02 22:28:07,350 main.py]: Epoch [23/50],  epoch_train_loss: 0.3226, epoch_val_loss: 0.4021, Train_Acc: 0.9903,  Val_Acc: 0.9016, Val_F1: 0.9016, Val_Pre: 0.9028, Val_Rec: 0.9024.
[2022-08-02 22:28:34,085 main.py]: Epoch [24/50],  epoch_train_loss: 0.3224, epoch_val_loss: 0.4272, Train_Acc: 0.9910,  Val_Acc: 0.8790, Val_F1: 0.8789, Val_Pre: 0.8846, Val_Rec: 0.8807.
[2022-08-02 22:29:00,802 main.py]: Epoch [25/50],  epoch_train_loss: 0.3205, epoch_val_loss: 0.4172, Train_Acc: 0.9929,  Val_Acc: 0.8888, Val_F1: 0.8888, Val_Pre: 0.8895, Val_Rec: 0.8894.
[2022-08-02 22:29:27,520 main.py]: Epoch [26/50],  epoch_train_loss: 0.3201, epoch_val_loss: 0.4066, Train_Acc: 0.9935,  Val_Acc: 0.8948, Val_F1: 0.8948, Val_Pre: 0.8949, Val_Rec: 0.8951.
[2022-08-02 22:29:54,191 main.py]: Epoch [27/50],  epoch_train_loss: 0.3196, epoch_val_loss: 0.4297, Train_Acc: 0.9941,  Val_Acc: 0.8798, Val_F1: 0.8796, Val_Pre: 0.8852, Val_Rec: 0.8814.
[2022-08-02 22:30:20,841 main.py]: Epoch [28/50],  epoch_train_loss: 0.3200, epoch_val_loss: 0.4171, Train_Acc: 0.9933,  Val_Acc: 0.8911, Val_F1: 0.8910, Val_Pre: 0.8936, Val_Rec: 0.8922.
[2022-08-02 22:30:47,463 main.py]: Epoch [29/50],  epoch_train_loss: 0.3205, epoch_val_loss: 0.4318, Train_Acc: 0.9931,  Val_Acc: 0.8790, Val_F1: 0.8789, Val_Pre: 0.8846, Val_Rec: 0.8807.
[2022-08-02 22:31:14,208 main.py]: Epoch [30/50],  epoch_train_loss: 0.3201, epoch_val_loss: 0.4392, Train_Acc: 0.9927,  Val_Acc: 0.8738, Val_F1: 0.8737, Val_Pre: 0.8779, Val_Rec: 0.8752.
[2022-08-02 22:31:40,854 main.py]: Epoch [31/50],  epoch_train_loss: 0.3208, epoch_val_loss: 0.4455, Train_Acc: 0.9927,  Val_Acc: 0.8678, Val_F1: 0.8674, Val_Pre: 0.8765, Val_Rec: 0.8698.
[2022-08-02 22:32:07,556 main.py]: Epoch [32/50],  epoch_train_loss: 0.3199, epoch_val_loss: 0.4099, Train_Acc: 0.9933,  Val_Acc: 0.8978, Val_F1: 0.8978, Val_Pre: 0.8986, Val_Rec: 0.8985.
[2022-08-02 22:32:34,272 main.py]: Epoch [33/50],  epoch_train_loss: 0.3204, epoch_val_loss: 0.4225, Train_Acc: 0.9929,  Val_Acc: 0.8896, Val_F1: 0.8895, Val_Pre: 0.8914, Val_Rec: 0.8905.
[2022-08-02 22:33:00,974 main.py]: Epoch [34/50],  epoch_train_loss: 0.3194, epoch_val_loss: 0.4251, Train_Acc: 0.9937,  Val_Acc: 0.8873, Val_F1: 0.8872, Val_Pre: 0.8912, Val_Rec: 0.8887.
Checkpoint Directory exists!
[2022-08-02 22:33:27,708 main.py]: Epoch [35/50],  epoch_train_loss: 0.3198, epoch_val_loss: 0.3859, Train_Acc: 0.9935,  Val_Acc: 0.9211, Val_F1: 0.9210, Val_Pre: 0.9215, Val_Rec: 0.9208.
[2022-08-02 22:34:07,524 main.py]: Epoch [36/50],  epoch_train_loss: 0.3204, epoch_val_loss: 0.4051, Train_Acc: 0.9929,  Val_Acc: 0.9053, Val_F1: 0.9053, Val_Pre: 0.9064, Val_Rec: 0.9061.
[2022-08-02 22:34:34,229 main.py]: Epoch [37/50],  epoch_train_loss: 0.3223, epoch_val_loss: 0.3927, Train_Acc: 0.9910,  Val_Acc: 0.9128, Val_F1: 0.9128, Val_Pre: 0.9129, Val_Rec: 0.9132.
[2022-08-02 22:35:00,983 main.py]: Epoch [38/50],  epoch_train_loss: 0.3210, epoch_val_loss: 0.4124, Train_Acc: 0.9922,  Val_Acc: 0.8888, Val_F1: 0.8888, Val_Pre: 0.8896, Val_Rec: 0.8895.
[2022-08-02 22:35:27,722 main.py]: Epoch [39/50],  epoch_train_loss: 0.3208, epoch_val_loss: 0.4364, Train_Acc: 0.9924,  Val_Acc: 0.8700, Val_F1: 0.8697, Val_Pre: 0.8776, Val_Rec: 0.8719.
[2022-08-02 22:35:54,530 main.py]: Epoch [40/50],  epoch_train_loss: 0.3229, epoch_val_loss: 0.4124, Train_Acc: 0.9903,  Val_Acc: 0.8971, Val_F1: 0.8971, Val_Pre: 0.8980, Val_Rec: 0.8978.
[2022-08-02 22:36:21,297 main.py]: Epoch [41/50],  epoch_train_loss: 0.3219, epoch_val_loss: 0.4150, Train_Acc: 0.9912,  Val_Acc: 0.8933, Val_F1: 0.8933, Val_Pre: 0.8960, Val_Rec: 0.8945.
[2022-08-02 22:36:48,074 main.py]: Epoch [42/50],  epoch_train_loss: 0.3217, epoch_val_loss: 0.4379, Train_Acc: 0.9914,  Val_Acc: 0.8738, Val_F1: 0.8733, Val_Pre: 0.8838, Val_Rec: 0.8759.
[2022-08-02 22:37:14,837 main.py]: Epoch [43/50],  epoch_train_loss: 0.3224, epoch_val_loss: 0.3940, Train_Acc: 0.9903,  Val_Acc: 0.9098, Val_F1: 0.9098, Val_Pre: 0.9097, Val_Rec: 0.9099.
[2022-08-02 22:37:41,633 main.py]: Epoch [44/50],  epoch_train_loss: 0.3272, epoch_val_loss: 0.4060, Train_Acc: 0.9857,  Val_Acc: 0.8978, Val_F1: 0.8976, Val_Pre: 0.8983, Val_Rec: 0.8974.
[2022-08-02 22:38:08,387 main.py]: Epoch [45/50],  epoch_train_loss: 0.3244, epoch_val_loss: 0.4194, Train_Acc: 0.9880,  Val_Acc: 0.8918, Val_F1: 0.8917, Val_Pre: 0.8965, Val_Rec: 0.8933.
[2022-08-02 22:38:35,059 main.py]: Epoch [46/50],  epoch_train_loss: 0.3235, epoch_val_loss: 0.4204, Train_Acc: 0.9893,  Val_Acc: 0.8911, Val_F1: 0.8909, Val_Pre: 0.8959, Val_Rec: 0.8926.
[2022-08-02 22:39:01,699 main.py]: Epoch [47/50],  epoch_train_loss: 0.3206, epoch_val_loss: 0.4250, Train_Acc: 0.9929,  Val_Acc: 0.8881, Val_F1: 0.8879, Val_Pre: 0.8929, Val_Rec: 0.8896.
[2022-08-02 22:39:28,300 main.py]: Epoch [48/50],  epoch_train_loss: 0.3185, epoch_val_loss: 0.4089, Train_Acc: 0.9950,  Val_Acc: 0.9038, Val_F1: 0.9038, Val_Pre: 0.9046, Val_Rec: 0.9045.
[2022-08-02 22:39:54,914 main.py]: Epoch [49/50],  epoch_train_loss: 0.3191, epoch_val_loss: 0.4154, Train_Acc: 0.9945,  Val_Acc: 0.8978, Val_F1: 0.8978, Val_Pre: 0.8997, Val_Rec: 0.8988.
[2022-08-02 22:40:21,542 main.py]: Epoch [50/50],  epoch_train_loss: 0.3186, epoch_val_loss: 0.4072, Train_Acc: 0.9948,  Val_Acc: 0.8993, Val_F1: 0.8993, Val_Pre: 0.9000, Val_Rec: 0.9000.
[2022-08-02 22:40:21,542 main.py]: start testing model
[2022-08-02 22:40:22,182 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/config.json from cache at /home/v-zuangao/.cache/torch/transformers/3227a4c62d38cb0f891e7ccb8fc0b7b7dbd873889b3688c2e95834fb6f6109be.573c56a39f56f50b7bc6e9803d92871c74059419e37e87c5b14d49b1546b0703
[2022-08-02 22:40:22,183 configuration_utils.py]: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
[2022-08-02 22:40:22,318 modeling_utils.py]: loading weights file https://cdn.huggingface.co/hfl/chinese-roberta-wwm-ext/pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/47d2326d47246cef3121d70d592c0391a4ed594b04ce3dea8bd47edd37e20370.6ac27309c356295f0e005c6029fce503ec6a32853911ebf79f8bddd8dd10edad
[2022-08-02 22:40:28,703 main.py]: showing the cache model metrics
[2022-08-02 22:40:28,703 main.py]: Classification Acc: 0.9211, F1: 0.9210, Precision: 0.9215, Recall: 0.9208, AUC-ROC: 0.9676
[2022-08-02 22:40:28,706 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.93      0.91      0.92       648
           1       0.91      0.93      0.92       683
    accuracy                           0.92      1331
   macro avg       0.92      0.92      0.92      1331
weighted avg       0.92      0.92      0.92      1331
[2022-08-02 22:40:28,707 main.py]: Classification confusion matrix:
[[588  60]
 [ 45 638]]