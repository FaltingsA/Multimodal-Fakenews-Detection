[2022-07-31 17:12:32,806 main.py: loading data
[2022-07-31 17:12:33,441 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-07-31 17:12:33,441 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-07-31 17:12:45,204 main.py: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-07-31 17:12:45,267 main.py: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-07-31 17:12:45,267 main.py: building model
[2022-07-31 17:12:45,900 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-07-31 17:12:45,900 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-07-31 17:12:45,929 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-07-31 17:12:48,208 main.py: expType: text, train in expCode: text_epoch50, batch_size 256, epoch = 1, lr = 0.0001
[2022-07-31 17:12:49,035 main.py: start training model
[2022-07-31 17:13:37,450 main.py: Epoch [1/1],  epoch_train_loss: 0.6936, epoch_val_loss: 0.6662, Train_Acc: 0.5090,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-07-31 17:13:38,119 main.py: start testing model
[2022-07-31 17:13:38,735 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-07-31 17:13:38,735 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-07-31 17:13:38,963 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-07-31 17:13:46,231 main.py: Classification Acc: 0.7167, F1: 0.4175, Precision: 0.3583, Recall: 0.5000, AUC-ROC: 0.6748
[2022-07-31 17:13:46,234 main.py: Classification report:
              precision    recall  f1-score   support
           0       0.72      1.00      0.83       989
           1       0.00      0.00      0.00       391
    accuracy                           0.72      1380
   macro avg       0.36      0.50      0.42      1380
weighted avg       0.51      0.72      0.60      1380
[2022-07-31 17:13:46,235 main.py: Classification confusion matrix:
[[989   0]
 [391   0]]