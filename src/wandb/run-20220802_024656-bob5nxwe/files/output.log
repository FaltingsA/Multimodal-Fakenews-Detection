[2022-08-02 02:46:58,557 main.py]: loading data
[2022-08-02 02:46:59,287 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-08-02 02:46:59,287 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
[2022-08-02 02:47:10,578 main.py]: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-08-02 02:47:10,640 main.py]: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 02:47:10,640 main.py]: building model
[2022-08-02 02:47:11,214 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 02:47:11,214 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 02:47:11,256 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 02:47:13,522 main.py]: expType: text, train in expCode: text_epoch50_freeze, batch_size 256, epoch = 50, lr = 0.0001
[2022-08-02 02:47:14,372 main.py]: start training model
[2022-08-02 02:48:03,265 main.py]: Epoch [1/50],  epoch_train_loss: 0.6936, epoch_val_loss: 0.6662, Train_Acc: 0.5090,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:48:54,304 main.py]: Epoch [2/50],  epoch_train_loss: 0.6918, epoch_val_loss: 0.6724, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:49:45,073 main.py]: Epoch [3/50],  epoch_train_loss: 0.6916, epoch_val_loss: 0.6736, Train_Acc: 0.5295,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:50:35,836 main.py]: Epoch [4/50],  epoch_train_loss: 0.6917, epoch_val_loss: 0.6687, Train_Acc: 0.5300,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:51:26,621 main.py]: Epoch [5/50],  epoch_train_loss: 0.6918, epoch_val_loss: 0.6660, Train_Acc: 0.5299,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:52:17,292 main.py]: Epoch [6/50],  epoch_train_loss: 0.6917, epoch_val_loss: 0.6683, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:53:07,866 main.py]: Epoch [7/50],  epoch_train_loss: 0.6914, epoch_val_loss: 0.6783, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:53:58,410 main.py]: Epoch [8/50],  epoch_train_loss: 0.6913, epoch_val_loss: 0.6685, Train_Acc: 0.5296,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:54:48,757 main.py]: Epoch [9/50],  epoch_train_loss: 0.6909, epoch_val_loss: 0.6693, Train_Acc: 0.5300,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:55:39,435 main.py]: Epoch [10/50],  epoch_train_loss: 0.6909, epoch_val_loss: 0.6663, Train_Acc: 0.5295,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:56:29,872 main.py]: Epoch [11/50],  epoch_train_loss: 0.6913, epoch_val_loss: 0.6622, Train_Acc: 0.5303,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:57:20,170 main.py]: Epoch [12/50],  epoch_train_loss: 0.6906, epoch_val_loss: 0.6731, Train_Acc: 0.5298,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:58:10,416 main.py]: Epoch [13/50],  epoch_train_loss: 0.6910, epoch_val_loss: 0.6758, Train_Acc: 0.5299,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:59:00,639 main.py]: Epoch [14/50],  epoch_train_loss: 0.6907, epoch_val_loss: 0.6727, Train_Acc: 0.5296,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 02:59:51,063 main.py]: Epoch [15/50],  epoch_train_loss: 0.6899, epoch_val_loss: 0.6770, Train_Acc: 0.5320,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:00:41,480 main.py]: Epoch [16/50],  epoch_train_loss: 0.6899, epoch_val_loss: 0.6699, Train_Acc: 0.5304,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:01:31,893 main.py]: Epoch [17/50],  epoch_train_loss: 0.6895, epoch_val_loss: 0.6804, Train_Acc: 0.5293,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:02:22,151 main.py]: Epoch [18/50],  epoch_train_loss: 0.6893, epoch_val_loss: 0.6757, Train_Acc: 0.5308,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:03:12,237 main.py]: Epoch [19/50],  epoch_train_loss: 0.6894, epoch_val_loss: 0.6743, Train_Acc: 0.5304,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:04:02,654 main.py]: Epoch [20/50],  epoch_train_loss: 0.6889, epoch_val_loss: 0.6654, Train_Acc: 0.5313,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:04:53,197 main.py]: Epoch [21/50],  epoch_train_loss: 0.6888, epoch_val_loss: 0.6652, Train_Acc: 0.5303,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:05:43,780 main.py]: Epoch [22/50],  epoch_train_loss: 0.6890, epoch_val_loss: 0.6799, Train_Acc: 0.5300,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:06:34,060 main.py]: Epoch [23/50],  epoch_train_loss: 0.6884, epoch_val_loss: 0.6820, Train_Acc: 0.5317,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 03:07:24,165 main.py]: Epoch [24/50],  epoch_train_loss: 0.6878, epoch_val_loss: 0.6693, Train_Acc: 0.5310,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:08:14,340 main.py]: Epoch [25/50],  epoch_train_loss: 0.6879, epoch_val_loss: 0.6662, Train_Acc: 0.5310,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:09:04,694 main.py]: Epoch [26/50],  epoch_train_loss: 0.6876, epoch_val_loss: 0.6582, Train_Acc: 0.5358,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:09:55,105 main.py]: Epoch [27/50],  epoch_train_loss: 0.6872, epoch_val_loss: 0.6670, Train_Acc: 0.5333,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:10:45,472 main.py]: Epoch [28/50],  epoch_train_loss: 0.6873, epoch_val_loss: 0.6695, Train_Acc: 0.5310,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:11:35,734 main.py]: Epoch [29/50],  epoch_train_loss: 0.6871, epoch_val_loss: 0.6752, Train_Acc: 0.5329,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 03:12:25,938 main.py]: Epoch [30/50],  epoch_train_loss: 0.6868, epoch_val_loss: 0.6666, Train_Acc: 0.5354,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:13:15,966 main.py]: Epoch [31/50],  epoch_train_loss: 0.6862, epoch_val_loss: 0.6702, Train_Acc: 0.5361,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:14:06,211 main.py]: Epoch [32/50],  epoch_train_loss: 0.6862, epoch_val_loss: 0.6775, Train_Acc: 0.5340,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 03:14:56,681 main.py]: Epoch [33/50],  epoch_train_loss: 0.6854, epoch_val_loss: 0.6693, Train_Acc: 0.5368,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:15:47,024 main.py]: Epoch [34/50],  epoch_train_loss: 0.6857, epoch_val_loss: 0.6735, Train_Acc: 0.5364,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 03:16:37,456 main.py]: Epoch [35/50],  epoch_train_loss: 0.6854, epoch_val_loss: 0.6780, Train_Acc: 0.5434,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 03:17:27,629 main.py]: Epoch [36/50],  epoch_train_loss: 0.6856, epoch_val_loss: 0.6917, Train_Acc: 0.5482,  Val_Acc: 0.3739, Val_F1: 0.3406, Val_Pre: 0.3704, Val_Rec: 0.3405.
[2022-08-02 03:18:17,763 main.py]: Epoch [37/50],  epoch_train_loss: 0.6848, epoch_val_loss: 0.6730, Train_Acc: 0.5392,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 03:19:08,110 main.py]: Epoch [38/50],  epoch_train_loss: 0.6846, epoch_val_loss: 0.6840, Train_Acc: 0.5478,  Val_Acc: 0.7275, Val_F1: 0.4741, Val_Pre: 0.7346, Val_Rec: 0.5254.
[2022-08-02 03:19:59,103 main.py]: Epoch [39/50],  epoch_train_loss: 0.6844, epoch_val_loss: 0.6923, Train_Acc: 0.5430,  Val_Acc: 0.3739, Val_F1: 0.3486, Val_Pre: 0.3822, Val_Rec: 0.3552.
[2022-08-02 03:20:49,588 main.py]: Epoch [40/50],  epoch_train_loss: 0.6836, epoch_val_loss: 0.6582, Train_Acc: 0.5510,  Val_Acc: 0.7167, Val_F1: 0.4175, Val_Pre: 0.3583, Val_Rec: 0.5000.
[2022-08-02 03:21:40,292 main.py]: Epoch [41/50],  epoch_train_loss: 0.6832, epoch_val_loss: 0.6808, Train_Acc: 0.5470,  Val_Acc: 0.7225, Val_F1: 0.4458, Val_Pre: 0.7538, Val_Rec: 0.5125.
[2022-08-02 03:22:30,962 main.py]: Epoch [42/50],  epoch_train_loss: 0.6835, epoch_val_loss: 0.6812, Train_Acc: 0.5570,  Val_Acc: 0.7254, Val_F1: 0.4561, Val_Pre: 0.7786, Val_Rec: 0.5177.
[2022-08-02 03:23:21,735 main.py]: Epoch [43/50],  epoch_train_loss: 0.6833, epoch_val_loss: 0.6746, Train_Acc: 0.5450,  Val_Acc: 0.7167, Val_F1: 0.4200, Val_Pre: 0.6085, Val_Rec: 0.5008.
[2022-08-02 03:24:12,537 main.py]: Epoch [44/50],  epoch_train_loss: 0.6829, epoch_val_loss: 0.6771, Train_Acc: 0.5551,  Val_Acc: 0.7167, Val_F1: 0.4248, Val_Pre: 0.6088, Val_Rec: 0.5023.
[2022-08-02 03:25:03,216 main.py]: Epoch [45/50],  epoch_train_loss: 0.6819, epoch_val_loss: 0.6727, Train_Acc: 0.5504,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 03:25:53,911 main.py]: Epoch [46/50],  epoch_train_loss: 0.6820, epoch_val_loss: 0.6851, Train_Acc: 0.5564,  Val_Acc: 0.6188, Val_F1: 0.4638, Val_Pre: 0.4655, Val_Rec: 0.4750.
[2022-08-02 03:26:44,636 main.py]: Epoch [47/50],  epoch_train_loss: 0.6816, epoch_val_loss: 0.6742, Train_Acc: 0.5511,  Val_Acc: 0.7174, Val_F1: 0.4251, Val_Pre: 0.6589, Val_Rec: 0.5028.
[2022-08-02 03:27:35,090 main.py]: Epoch [48/50],  epoch_train_loss: 0.6812, epoch_val_loss: 0.6572, Train_Acc: 0.5611,  Val_Acc: 0.7159, Val_F1: 0.4172, Val_Pre: 0.3582, Val_Rec: 0.4995.
[2022-08-02 03:28:25,698 main.py]: Epoch [49/50],  epoch_train_loss: 0.6821, epoch_val_loss: 0.6896, Train_Acc: 0.5532,  Val_Acc: 0.3891, Val_F1: 0.3528, Val_Pre: 0.3801, Val_Rec: 0.3527.
[2022-08-02 03:29:16,362 main.py]: Epoch [50/50],  epoch_train_loss: 0.6804, epoch_val_loss: 0.6770, Train_Acc: 0.5618,  Val_Acc: 0.7254, Val_F1: 0.4626, Val_Pre: 0.7375, Val_Rec: 0.5200.
[2022-08-02 03:29:16,362 main.py]: start testing model
[2022-08-02 03:29:16,947 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 03:29:16,947 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 03:29:17,106 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-02 03:29:20,007 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 03:29:20,007 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 03:29:20,048 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
--------------------------------------------------------------------------------
[2022-08-02 03:29:27,404 main.py]: Classification Acc: 0.7275, F1: 0.4741, Precision: 0.7346, Recall: 0.5254, AUC-ROC: 0.4081
[2022-08-02 03:29:27,407 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.73      0.99      0.84       989
           1       0.74      0.06      0.11       391
    accuracy                           0.73      1380
   macro avg       0.73      0.53      0.47      1380
weighted avg       0.73      0.73      0.63      1380
[2022-08-02 03:29:27,407 main.py]: Classification confusion matrix:
[[981   8]
 [368  23]]
[2022-08-02 03:29:31,819 main.py]: Classification Acc: 0.7275, F1: 0.4741, Precision: 0.7346, Recall: 0.5254, AUC-ROC: 0.4081
[2022-08-02 03:29:31,822 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.73      0.99      0.84       989
           1       0.74      0.06      0.11       391
    accuracy                           0.73      1380
   macro avg       0.73      0.53      0.47      1380
weighted avg       0.73      0.73      0.63      1380
[2022-08-02 03:29:31,822 main.py]: Classification confusion matrix:
[[981   8]
 [368  23]]