[2022-08-02 05:26:52,539 main.py]: loading data
[2022-08-02 05:26:53,183 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-08-02 05:26:53,183 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
[2022-08-02 05:27:04,068 main.py]: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-08-02 05:27:04,125 main.py]: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 05:27:04,126 main.py]: building model
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 05:27:04,723 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 05:27:04,723 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 05:27:04,761 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-02 05:27:07,043 main.py]: expType: all, train in expCode: all_epoch50_freeze, batch_size 256, epoch = 50, lr = 0.0001
[2022-08-02 05:27:07,899 main.py]: start training model
[2022-08-02 05:28:30,501 main.py]: Epoch [1/50],  epoch_train_loss: 0.5196, epoch_val_loss: 0.4749, Train_Acc: 0.8442,  Val_Acc: 0.8246, Val_F1: 0.7230, Val_Pre: 0.8957, Val_Rec: 0.6921.
[2022-08-02 05:29:54,492 main.py]: Epoch [2/50],  epoch_train_loss: 0.3857, epoch_val_loss: 0.4111, Train_Acc: 0.9359,  Val_Acc: 0.8978, Val_F1: 0.8606, Val_Pre: 0.9185, Val_Rec: 0.8297.
[2022-08-02 05:31:18,505 main.py]: Epoch [3/50],  epoch_train_loss: 0.3717, epoch_val_loss: 0.5060, Train_Acc: 0.9438,  Val_Acc: 0.7478, Val_F1: 0.5243, Val_Pre: 0.8699, Val_Rec: 0.5550.
[2022-08-02 05:32:41,812 main.py]: Epoch [4/50],  epoch_train_loss: 0.3689, epoch_val_loss: 0.4362, Train_Acc: 0.9453,  Val_Acc: 0.9043, Val_F1: 0.8712, Val_Pre: 0.9204, Val_Rec: 0.8428.
[2022-08-02 05:34:05,784 main.py]: Epoch [5/50],  epoch_train_loss: 0.3644, epoch_val_loss: 0.5317, Train_Acc: 0.9485,  Val_Acc: 0.7471, Val_F1: 0.5256, Val_Pre: 0.8482, Val_Rec: 0.5553.
[2022-08-02 05:35:28,924 main.py]: Epoch [6/50],  epoch_train_loss: 0.3632, epoch_val_loss: 0.4821, Train_Acc: 0.9497,  Val_Acc: 0.7913, Val_F1: 0.6515, Val_Pre: 0.8625, Val_Rec: 0.6364.
[2022-08-02 05:36:52,319 main.py]: Epoch [7/50],  epoch_train_loss: 0.3625, epoch_val_loss: 0.4985, Train_Acc: 0.9510,  Val_Acc: 0.7659, Val_F1: 0.5780, Val_Pre: 0.8769, Val_Rec: 0.5870.
[2022-08-02 05:38:15,270 main.py]: Epoch [8/50],  epoch_train_loss: 0.3622, epoch_val_loss: 0.4587, Train_Acc: 0.9514,  Val_Acc: 0.8283, Val_F1: 0.7298, Val_Pre: 0.9004, Val_Rec: 0.6977.
[2022-08-02 05:39:39,042 main.py]: Epoch [9/50],  epoch_train_loss: 0.3612, epoch_val_loss: 0.5172, Train_Acc: 0.9523,  Val_Acc: 0.7623, Val_F1: 0.5794, Val_Pre: 0.8264, Val_Rec: 0.5867.
[2022-08-02 05:41:02,246 main.py]: Epoch [10/50],  epoch_train_loss: 0.3612, epoch_val_loss: 0.5240, Train_Acc: 0.9525,  Val_Acc: 0.7601, Val_F1: 0.5705, Val_Pre: 0.8341, Val_Rec: 0.5814.
[2022-08-02 05:42:25,478 main.py]: Epoch [11/50],  epoch_train_loss: 0.3611, epoch_val_loss: 0.5223, Train_Acc: 0.9521,  Val_Acc: 0.7500, Val_F1: 0.5584, Val_Pre: 0.7676, Val_Rec: 0.5720.
[2022-08-02 05:43:48,310 main.py]: Epoch [12/50],  epoch_train_loss: 0.3608, epoch_val_loss: 0.5410, Train_Acc: 0.9526,  Val_Acc: 0.7319, Val_F1: 0.5035, Val_Pre: 0.7157, Val_Rec: 0.5392.
[2022-08-02 05:45:11,176 main.py]: Epoch [13/50],  epoch_train_loss: 0.3611, epoch_val_loss: 0.4738, Train_Acc: 0.9525,  Val_Acc: 0.8072, Val_F1: 0.6921, Val_Pre: 0.8619, Val_Rec: 0.6676.
[2022-08-02 05:46:34,594 main.py]: Epoch [14/50],  epoch_train_loss: 0.3600, epoch_val_loss: 0.4662, Train_Acc: 0.9533,  Val_Acc: 0.8188, Val_F1: 0.7214, Val_Pre: 0.8568, Val_Rec: 0.6927.
[2022-08-02 05:47:57,453 main.py]: Epoch [15/50],  epoch_train_loss: 0.3598, epoch_val_loss: 0.5166, Train_Acc: 0.9540,  Val_Acc: 0.7587, Val_F1: 0.5664, Val_Pre: 0.8323, Val_Rec: 0.5788.
[2022-08-02 05:49:20,194 main.py]: Epoch [16/50],  epoch_train_loss: 0.3598, epoch_val_loss: 0.4543, Train_Acc: 0.9539,  Val_Acc: 0.8399, Val_F1: 0.7650, Val_Pre: 0.8665, Val_Rec: 0.7329.
[2022-08-02 05:50:43,218 main.py]: Epoch [17/50],  epoch_train_loss: 0.3599, epoch_val_loss: 0.4913, Train_Acc: 0.9540,  Val_Acc: 0.8167, Val_F1: 0.7290, Val_Pre: 0.8256, Val_Rec: 0.7020.
[2022-08-02 05:52:06,265 main.py]: Epoch [18/50],  epoch_train_loss: 0.3590, epoch_val_loss: 0.5323, Train_Acc: 0.9540,  Val_Acc: 0.7529, Val_F1: 0.5415, Val_Pre: 0.8624, Val_Rec: 0.5647.
[2022-08-02 05:53:29,552 main.py]: Epoch [19/50],  epoch_train_loss: 0.3598, epoch_val_loss: 0.4699, Train_Acc: 0.9542,  Val_Acc: 0.8384, Val_F1: 0.7673, Val_Pre: 0.8518, Val_Rec: 0.7373.
[2022-08-02 05:54:52,400 main.py]: Epoch [20/50],  epoch_train_loss: 0.3588, epoch_val_loss: 0.5301, Train_Acc: 0.9539,  Val_Acc: 0.7558, Val_F1: 0.5550, Val_Pre: 0.8414, Val_Rec: 0.5721.
[2022-08-02 05:56:15,464 main.py]: Epoch [21/50],  epoch_train_loss: 0.3602, epoch_val_loss: 0.5335, Train_Acc: 0.9536,  Val_Acc: 0.7486, Val_F1: 0.5558, Val_Pre: 0.7610, Val_Rec: 0.5702.
[2022-08-02 05:57:38,097 main.py]: Epoch [22/50],  epoch_train_loss: 0.3598, epoch_val_loss: 0.5307, Train_Acc: 0.9543,  Val_Acc: 0.7558, Val_F1: 0.5812, Val_Pre: 0.7649, Val_Rec: 0.5861.
[2022-08-02 05:59:01,084 main.py]: Epoch [23/50],  epoch_train_loss: 0.3593, epoch_val_loss: 0.5462, Train_Acc: 0.9541,  Val_Acc: 0.7565, Val_F1: 0.5884, Val_Pre: 0.7567, Val_Rec: 0.5904.
[2022-08-02 06:00:24,453 main.py]: Epoch [24/50],  epoch_train_loss: 0.3587, epoch_val_loss: 0.5734, Train_Acc: 0.9543,  Val_Acc: 0.7217, Val_F1: 0.6144, Val_Pre: 0.6419, Val_Rec: 0.6071.
[2022-08-02 06:01:47,139 main.py]: Epoch [25/50],  epoch_train_loss: 0.3593, epoch_val_loss: 0.5233, Train_Acc: 0.9543,  Val_Acc: 0.7630, Val_F1: 0.6000, Val_Pre: 0.7783, Val_Rec: 0.5989.
[2022-08-02 06:03:10,749 main.py]: Epoch [26/50],  epoch_train_loss: 0.3595, epoch_val_loss: 0.5226, Train_Acc: 0.9542,  Val_Acc: 0.7732, Val_F1: 0.6242, Val_Pre: 0.7966, Val_Rec: 0.6160.
[2022-08-02 06:04:33,954 main.py]: Epoch [27/50],  epoch_train_loss: 0.3592, epoch_val_loss: 0.5413, Train_Acc: 0.9543,  Val_Acc: 0.7384, Val_F1: 0.5037, Val_Pre: 0.8047, Val_Rec: 0.5422.
[2022-08-02 06:05:57,523 main.py]: Epoch [28/50],  epoch_train_loss: 0.3591, epoch_val_loss: 0.5571, Train_Acc: 0.9545,  Val_Acc: 0.7362, Val_F1: 0.6860, Val_Pre: 0.6811, Val_Rec: 0.6930.
[2022-08-02 06:07:20,314 main.py]: Epoch [29/50],  epoch_train_loss: 0.3585, epoch_val_loss: 0.5260, Train_Acc: 0.9545,  Val_Acc: 0.7616, Val_F1: 0.5731, Val_Pre: 0.8414, Val_Rec: 0.5831.
[2022-08-02 06:08:43,124 main.py]: Epoch [30/50],  epoch_train_loss: 0.3593, epoch_val_loss: 0.5304, Train_Acc: 0.9545,  Val_Acc: 0.7514, Val_F1: 0.5487, Val_Pre: 0.8102, Val_Rec: 0.5676.
[2022-08-02 06:10:05,843 main.py]: Epoch [31/50],  epoch_train_loss: 0.3581, epoch_val_loss: 0.5350, Train_Acc: 0.9546,  Val_Acc: 0.7442, Val_F1: 0.5482, Val_Pre: 0.7411, Val_Rec: 0.5648.
[2022-08-02 06:11:28,571 main.py]: Epoch [32/50],  epoch_train_loss: 0.3590, epoch_val_loss: 0.5088, Train_Acc: 0.9541,  Val_Acc: 0.7710, Val_F1: 0.6300, Val_Pre: 0.7722, Val_Rec: 0.6199.
[2022-08-02 06:12:51,473 main.py]: Epoch [33/50],  epoch_train_loss: 0.3633, epoch_val_loss: 0.5267, Train_Acc: 0.9505,  Val_Acc: 0.7594, Val_F1: 0.5592, Val_Pre: 0.8743, Val_Rec: 0.5754.
[2022-08-02 06:14:14,415 main.py]: Epoch [34/50],  epoch_train_loss: 0.3584, epoch_val_loss: 0.5241, Train_Acc: 0.9547,  Val_Acc: 0.7638, Val_F1: 0.5916, Val_Pre: 0.8035, Val_Rec: 0.5939.
[2022-08-02 06:15:37,301 main.py]: Epoch [35/50],  epoch_train_loss: 0.3585, epoch_val_loss: 0.5253, Train_Acc: 0.9548,  Val_Acc: 0.7594, Val_F1: 0.5670, Val_Pre: 0.8391, Val_Rec: 0.5793.
[2022-08-02 06:17:00,002 main.py]: Epoch [36/50],  epoch_train_loss: 0.3591, epoch_val_loss: 0.5186, Train_Acc: 0.9548,  Val_Acc: 0.7710, Val_F1: 0.6015, Val_Pre: 0.8410, Val_Rec: 0.6013.
[2022-08-02 06:18:22,779 main.py]: Epoch [37/50],  epoch_train_loss: 0.3591, epoch_val_loss: 0.5193, Train_Acc: 0.9548,  Val_Acc: 0.7710, Val_F1: 0.6153, Val_Pre: 0.8015, Val_Rec: 0.6098.
[2022-08-02 06:19:45,685 main.py]: Epoch [38/50],  epoch_train_loss: 0.3582, epoch_val_loss: 0.5220, Train_Acc: 0.9547,  Val_Acc: 0.7623, Val_F1: 0.5737, Val_Pre: 0.8481, Val_Rec: 0.5837.
[2022-08-02 06:21:08,735 main.py]: Epoch [39/50],  epoch_train_loss: 0.3582, epoch_val_loss: 0.5207, Train_Acc: 0.9548,  Val_Acc: 0.7710, Val_F1: 0.6188, Val_Pre: 0.7935, Val_Rec: 0.6121.
[2022-08-02 06:22:31,512 main.py]: Epoch [40/50],  epoch_train_loss: 0.3591, epoch_val_loss: 0.5239, Train_Acc: 0.9547,  Val_Acc: 0.7659, Val_F1: 0.5920, Val_Pre: 0.8221, Val_Rec: 0.5947.
[2022-08-02 06:23:54,119 main.py]: Epoch [41/50],  epoch_train_loss: 0.3585, epoch_val_loss: 0.5291, Train_Acc: 0.9548,  Val_Acc: 0.7551, Val_F1: 0.5695, Val_Pre: 0.7851, Val_Rec: 0.5794.
[2022-08-02 06:25:17,170 main.py]: Epoch [42/50],  epoch_train_loss: 0.3582, epoch_val_loss: 0.5284, Train_Acc: 0.9548,  Val_Acc: 0.7572, Val_F1: 0.5877, Val_Pre: 0.7628, Val_Rec: 0.5902.
[2022-08-02 06:26:40,305 main.py]: Epoch [43/50],  epoch_train_loss: 0.3585, epoch_val_loss: 0.5226, Train_Acc: 0.9548,  Val_Acc: 0.7674, Val_F1: 0.6145, Val_Pre: 0.7784, Val_Rec: 0.6088.
[2022-08-02 06:28:03,185 main.py]: Epoch [44/50],  epoch_train_loss: 0.3585, epoch_val_loss: 0.5236, Train_Acc: 0.9548,  Val_Acc: 0.7659, Val_F1: 0.5973, Val_Pre: 0.8068, Val_Rec: 0.5978.
[2022-08-02 06:29:26,368 main.py]: Epoch [45/50],  epoch_train_loss: 0.3582, epoch_val_loss: 0.5234, Train_Acc: 0.9548,  Val_Acc: 0.7674, Val_F1: 0.6061, Val_Pre: 0.7962, Val_Rec: 0.6034.
[2022-08-02 06:30:49,396 main.py]: Epoch [46/50],  epoch_train_loss: 0.3585, epoch_val_loss: 0.5175, Train_Acc: 0.9548,  Val_Acc: 0.7696, Val_F1: 0.6140, Val_Pre: 0.7939, Val_Rec: 0.6088.
[2022-08-02 06:32:12,920 main.py]: Epoch [47/50],  epoch_train_loss: 0.3583, epoch_val_loss: 0.5090, Train_Acc: 0.9546,  Val_Acc: 0.7833, Val_F1: 0.6272, Val_Pre: 0.8739, Val_Rec: 0.6192.
[2022-08-02 06:33:35,575 main.py]: Epoch [48/50],  epoch_train_loss: 0.3580, epoch_val_loss: 0.5091, Train_Acc: 0.9546,  Val_Acc: 0.7804, Val_F1: 0.6259, Val_Pre: 0.8499, Val_Rec: 0.6179.
[2022-08-02 06:34:58,802 main.py]: Epoch [49/50],  epoch_train_loss: 0.3591, epoch_val_loss: 0.5475, Train_Acc: 0.9544,  Val_Acc: 0.7703, Val_F1: 0.6425, Val_Pre: 0.7510, Val_Rec: 0.6294.
[2022-08-02 06:36:21,984 main.py]: Epoch [50/50],  epoch_train_loss: 0.3584, epoch_val_loss: 0.5027, Train_Acc: 0.9545,  Val_Acc: 0.7891, Val_F1: 0.6407, Val_Pre: 0.8816, Val_Rec: 0.6287.
[2022-08-02 06:36:21,984 main.py]: start testing model
[2022-08-02 06:36:22,601 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 06:36:22,601 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 06:36:22,715 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-02 06:36:25,727 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 06:36:25,727 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 06:36:25,755 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
--------------------------------------------------------------------------------
[2022-08-02 06:36:34,531 main.py]: Classification Acc: 0.9087, F1: 0.8780, Precision: 0.9223, Recall: 0.8512, AUC-ROC: 0.9110
[2022-08-02 06:36:34,534 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.90      0.98      0.94       989
           1       0.95      0.72      0.82       391
    accuracy                           0.91      1380
   macro avg       0.92      0.85      0.88      1380
weighted avg       0.91      0.91      0.90      1380
[2022-08-02 06:36:34,534 main.py]: Classification confusion matrix:
[[973  16]
 [110 281]]
[2022-08-02 06:36:40,366 main.py]: Classification Acc: 0.9051, F1: 0.8721, Precision: 0.9221, Recall: 0.8433, AUC-ROC: 0.9532
[2022-08-02 06:36:40,370 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.89      0.99      0.94       989
           1       0.95      0.70      0.81       391
    accuracy                           0.91      1380
   macro avg       0.92      0.84      0.87      1380
weighted avg       0.91      0.91      0.90      1380
[2022-08-02 06:36:40,370 main.py]: Classification confusion matrix:
[[975  14]
 [117 274]]