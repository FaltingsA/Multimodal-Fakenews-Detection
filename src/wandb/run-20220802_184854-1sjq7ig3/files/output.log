[2022-08-02 18:48:56,758 main.py]: loading data
[2022-08-02 18:48:57,401 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-08-02 18:48:57,401 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
[2022-08-02 18:49:08,473 main.py]: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-08-02 18:49:08,533 main.py]: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 18:49:08,533 main.py]: building model
[2022-08-02 18:49:09,154 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 18:49:09,154 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 18:49:09,183 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 18:49:11,477 main.py]: expType: mulT, train in expCode: mulT_epoch50_freeze, batch_size 256, epoch = 50, lr = 0.0001
[2022-08-02 18:49:12,408 main.py]: start training model
Checkpoint Directory exists!
[2022-08-02 18:50:48,522 main.py]: Epoch [1/50],  epoch_train_loss: 0.4505, epoch_val_loss: 0.4558, Train_Acc: 0.8708,  Val_Acc: 0.8138, Val_F1: 0.6986, Val_Pre: 0.8934, Val_Rec: 0.6721.
Checkpoint Directory exists!
[2022-08-02 18:52:27,047 main.py]: Epoch [2/50],  epoch_train_loss: 0.3684, epoch_val_loss: 0.4189, Train_Acc: 0.9459,  Val_Acc: 0.9217, Val_F1: 0.9064, Val_Pre: 0.8962, Val_Rec: 0.9191.
[2022-08-02 18:54:21,206 main.py]: Epoch [3/50],  epoch_train_loss: 0.3618, epoch_val_loss: 0.4237, Train_Acc: 0.9516,  Val_Acc: 0.8964, Val_F1: 0.8679, Val_Pre: 0.8831, Val_Rec: 0.8558.
[2022-08-02 18:55:58,269 main.py]: Epoch [4/50],  epoch_train_loss: 0.3599, epoch_val_loss: 0.4435, Train_Acc: 0.9536,  Val_Acc: 0.8667, Val_F1: 0.8255, Val_Pre: 0.8519, Val_Rec: 0.8080.
[2022-08-02 18:57:35,267 main.py]: Epoch [5/50],  epoch_train_loss: 0.3582, epoch_val_loss: 0.4622, Train_Acc: 0.9548,  Val_Acc: 0.8964, Val_F1: 0.8764, Val_Pre: 0.8664, Val_Rec: 0.8890.
[2022-08-02 18:59:11,642 main.py]: Epoch [6/50],  epoch_train_loss: 0.3569, epoch_val_loss: 0.4757, Train_Acc: 0.9561,  Val_Acc: 0.8080, Val_F1: 0.6954, Val_Pre: 0.8572, Val_Rec: 0.6704.
[2022-08-02 19:00:48,266 main.py]: Epoch [7/50],  epoch_train_loss: 0.3575, epoch_val_loss: 0.5721, Train_Acc: 0.9554,  Val_Acc: 0.7754, Val_F1: 0.7610, Val_Pre: 0.7640, Val_Rec: 0.8239.
[2022-08-02 19:02:24,793 main.py]: Epoch [8/50],  epoch_train_loss: 0.3567, epoch_val_loss: 0.4857, Train_Acc: 0.9565,  Val_Acc: 0.7920, Val_F1: 0.6595, Val_Pre: 0.8433, Val_Rec: 0.6423.
[2022-08-02 19:04:00,927 main.py]: Epoch [9/50],  epoch_train_loss: 0.3543, epoch_val_loss: 0.4250, Train_Acc: 0.9592,  Val_Acc: 0.8775, Val_F1: 0.8346, Val_Pre: 0.8814, Val_Rec: 0.8086.
[2022-08-02 19:05:37,873 main.py]: Epoch [10/50],  epoch_train_loss: 0.3553, epoch_val_loss: 0.4890, Train_Acc: 0.9580,  Val_Acc: 0.7884, Val_F1: 0.6400, Val_Pre: 0.8767, Val_Rec: 0.6281.
[2022-08-02 19:07:15,125 main.py]: Epoch [11/50],  epoch_train_loss: 0.3531, epoch_val_loss: 0.4496, Train_Acc: 0.9602,  Val_Acc: 0.8145, Val_F1: 0.7078, Val_Pre: 0.8673, Val_Rec: 0.6804.
[2022-08-02 19:08:51,648 main.py]: Epoch [12/50],  epoch_train_loss: 0.3543, epoch_val_loss: 0.4778, Train_Acc: 0.9587,  Val_Acc: 0.8007, Val_F1: 0.6916, Val_Pre: 0.8199, Val_Rec: 0.6684.
[2022-08-02 19:10:28,474 main.py]: Epoch [13/50],  epoch_train_loss: 0.3536, epoch_val_loss: 0.4831, Train_Acc: 0.9600,  Val_Acc: 0.8014, Val_F1: 0.6863, Val_Pre: 0.8369, Val_Rec: 0.6635.
[2022-08-02 19:12:05,066 main.py]: Epoch [14/50],  epoch_train_loss: 0.3506, epoch_val_loss: 0.4787, Train_Acc: 0.9622,  Val_Acc: 0.8522, Val_F1: 0.8104, Val_Pre: 0.8260, Val_Rec: 0.7987.
[2022-08-02 19:13:42,113 main.py]: Epoch [15/50],  epoch_train_loss: 0.3512, epoch_val_loss: 0.4941, Train_Acc: 0.9624,  Val_Acc: 0.7841, Val_F1: 0.6427, Val_Pre: 0.8294, Val_Rec: 0.6298.
[2022-08-02 19:15:18,438 main.py]: Epoch [16/50],  epoch_train_loss: 0.3516, epoch_val_loss: 0.5008, Train_Acc: 0.9621,  Val_Acc: 0.7732, Val_F1: 0.6778, Val_Pre: 0.7299, Val_Rec: 0.6616.
[2022-08-02 19:16:55,203 main.py]: Epoch [17/50],  epoch_train_loss: 0.3533, epoch_val_loss: 0.5400, Train_Acc: 0.9599,  Val_Acc: 0.8283, Val_F1: 0.8017, Val_Pre: 0.7894, Val_Rec: 0.8245.
[2022-08-02 19:18:32,124 main.py]: Epoch [18/50],  epoch_train_loss: 0.3518, epoch_val_loss: 0.4379, Train_Acc: 0.9611,  Val_Acc: 0.8406, Val_F1: 0.7680, Val_Pre: 0.8623, Val_Rec: 0.7365.
[2022-08-02 19:20:09,190 main.py]: Epoch [19/50],  epoch_train_loss: 0.3510, epoch_val_loss: 0.4842, Train_Acc: 0.9619,  Val_Acc: 0.8058, Val_F1: 0.6871, Val_Pre: 0.8665, Val_Rec: 0.6635.
[2022-08-02 19:21:45,571 main.py]: Epoch [20/50],  epoch_train_loss: 0.3490, epoch_val_loss: 0.5222, Train_Acc: 0.9638,  Val_Acc: 0.7312, Val_F1: 0.4821, Val_Pre: 0.7709, Val_Rec: 0.5302.
[2022-08-02 19:23:22,289 main.py]: Epoch [21/50],  epoch_train_loss: 0.3515, epoch_val_loss: 0.4812, Train_Acc: 0.9624,  Val_Acc: 0.8732, Val_F1: 0.8513, Val_Pre: 0.8386, Val_Rec: 0.8705.
[2022-08-02 19:24:58,429 main.py]: Epoch [22/50],  epoch_train_loss: 0.3500, epoch_val_loss: 0.4784, Train_Acc: 0.9633,  Val_Acc: 0.7826, Val_F1: 0.6403, Val_Pre: 0.8248, Val_Rec: 0.6280.
[2022-08-02 19:26:35,181 main.py]: Epoch [23/50],  epoch_train_loss: 0.3480, epoch_val_loss: 0.4896, Train_Acc: 0.9653,  Val_Acc: 0.7877, Val_F1: 0.6449, Val_Pre: 0.8524, Val_Rec: 0.6315.
[2022-08-02 19:28:11,792 main.py]: Epoch [24/50],  epoch_train_loss: 0.3486, epoch_val_loss: 0.4708, Train_Acc: 0.9643,  Val_Acc: 0.7891, Val_F1: 0.6578, Val_Pre: 0.8266, Val_Rec: 0.6410.
[2022-08-02 19:29:48,861 main.py]: Epoch [25/50],  epoch_train_loss: 0.3483, epoch_val_loss: 0.5531, Train_Acc: 0.9658,  Val_Acc: 0.7232, Val_F1: 0.4551, Val_Pre: 0.7188, Val_Rec: 0.5161.
[2022-08-02 19:31:25,190 main.py]: Epoch [26/50],  epoch_train_loss: 0.3464, epoch_val_loss: 0.5502, Train_Acc: 0.9674,  Val_Acc: 0.7246, Val_F1: 0.4622, Val_Pre: 0.7224, Val_Rec: 0.5195.
[2022-08-02 19:33:01,959 main.py]: Epoch [27/50],  epoch_train_loss: 0.3474, epoch_val_loss: 0.5473, Train_Acc: 0.9663,  Val_Acc: 0.7196, Val_F1: 0.4639, Val_Pre: 0.6398, Val_Rec: 0.5175.
[2022-08-02 19:34:38,586 main.py]: Epoch [28/50],  epoch_train_loss: 0.3479, epoch_val_loss: 0.4418, Train_Acc: 0.9653,  Val_Acc: 0.8630, Val_F1: 0.8277, Val_Pre: 0.8356, Val_Rec: 0.8209.
[2022-08-02 19:36:15,990 main.py]: Epoch [29/50],  epoch_train_loss: 0.3473, epoch_val_loss: 0.4773, Train_Acc: 0.9658,  Val_Acc: 0.8043, Val_F1: 0.6838, Val_Pre: 0.8654, Val_Rec: 0.6609.
[2022-08-02 19:37:52,682 main.py]: Epoch [30/50],  epoch_train_loss: 0.3498, epoch_val_loss: 0.4685, Train_Acc: 0.9643,  Val_Acc: 0.8304, Val_F1: 0.7386, Val_Pre: 0.8857, Val_Rec: 0.7062.
[2022-08-02 19:39:29,572 main.py]: Epoch [31/50],  epoch_train_loss: 0.3453, epoch_val_loss: 0.5008, Train_Acc: 0.9674,  Val_Acc: 0.7841, Val_F1: 0.6383, Val_Pre: 0.8421, Val_Rec: 0.6267.
[2022-08-02 19:41:06,202 main.py]: Epoch [32/50],  epoch_train_loss: 0.3456, epoch_val_loss: 0.5878, Train_Acc: 0.9673,  Val_Acc: 0.7014, Val_F1: 0.6400, Val_Pre: 0.6376, Val_Rec: 0.6433.
[2022-08-02 19:42:43,034 main.py]: Epoch [33/50],  epoch_train_loss: 0.3479, epoch_val_loss: 0.5492, Train_Acc: 0.9661,  Val_Acc: 0.7746, Val_F1: 0.7081, Val_Pre: 0.7220, Val_Rec: 0.6990.
[2022-08-02 19:44:19,449 main.py]: Epoch [34/50],  epoch_train_loss: 0.3481, epoch_val_loss: 0.4681, Train_Acc: 0.9650,  Val_Acc: 0.8464, Val_F1: 0.7971, Val_Pre: 0.8262, Val_Rec: 0.7792.
[2022-08-02 19:45:56,565 main.py]: Epoch [35/50],  epoch_train_loss: 0.3480, epoch_val_loss: 0.5096, Train_Acc: 0.9652,  Val_Acc: 0.8290, Val_F1: 0.8036, Val_Pre: 0.7908, Val_Rec: 0.8289.
[2022-08-02 19:47:33,161 main.py]: Epoch [36/50],  epoch_train_loss: 0.3507, epoch_val_loss: 0.4407, Train_Acc: 0.9633,  Val_Acc: 0.8623, Val_F1: 0.8092, Val_Pre: 0.8710, Val_Rec: 0.7802.
[2022-08-02 19:49:09,759 main.py]: Epoch [37/50],  epoch_train_loss: 0.3471, epoch_val_loss: 0.4601, Train_Acc: 0.9669,  Val_Acc: 0.8268, Val_F1: 0.7556, Val_Pre: 0.8208, Val_Rec: 0.7299.
[2022-08-02 19:50:46,299 main.py]: Epoch [38/50],  epoch_train_loss: 0.3466, epoch_val_loss: 0.4894, Train_Acc: 0.9665,  Val_Acc: 0.8362, Val_F1: 0.7928, Val_Pre: 0.8020, Val_Rec: 0.7852.
[2022-08-02 19:52:23,102 main.py]: Epoch [39/50],  epoch_train_loss: 0.3467, epoch_val_loss: 0.5192, Train_Acc: 0.9658,  Val_Acc: 0.7913, Val_F1: 0.7144, Val_Pre: 0.7526, Val_Rec: 0.6974.
[2022-08-02 19:53:59,518 main.py]: Epoch [40/50],  epoch_train_loss: 0.3442, epoch_val_loss: 0.5537, Train_Acc: 0.9686,  Val_Acc: 0.7246, Val_F1: 0.4535, Val_Pre: 0.7735, Val_Rec: 0.5164.
[2022-08-02 19:55:36,154 main.py]: Epoch [41/50],  epoch_train_loss: 0.3476, epoch_val_loss: 0.5555, Train_Acc: 0.9653,  Val_Acc: 0.7514, Val_F1: 0.5930, Val_Pre: 0.7227, Val_Rec: 0.5923.
[2022-08-02 19:57:12,388 main.py]: Epoch [42/50],  epoch_train_loss: 0.3454, epoch_val_loss: 0.5020, Train_Acc: 0.9675,  Val_Acc: 0.8261, Val_F1: 0.7938, Val_Pre: 0.7851, Val_Rec: 0.8060.
[2022-08-02 19:58:49,137 main.py]: Epoch [43/50],  epoch_train_loss: 0.3450, epoch_val_loss: 0.5657, Train_Acc: 0.9678,  Val_Acc: 0.7188, Val_F1: 0.4420, Val_Pre: 0.6544, Val_Rec: 0.5092.
[2022-08-02 20:00:25,692 main.py]: Epoch [44/50],  epoch_train_loss: 0.3446, epoch_val_loss: 0.5552, Train_Acc: 0.9693,  Val_Acc: 0.7290, Val_F1: 0.4748, Val_Pre: 0.7604, Val_Rec: 0.5264.
[2022-08-02 20:02:02,685 main.py]: Epoch [45/50],  epoch_train_loss: 0.3430, epoch_val_loss: 0.5251, Train_Acc: 0.9698,  Val_Acc: 0.7906, Val_F1: 0.6866, Val_Pre: 0.7790, Val_Rec: 0.6660.
[2022-08-02 20:03:39,098 main.py]: Epoch [46/50],  epoch_train_loss: 0.3438, epoch_val_loss: 0.5565, Train_Acc: 0.9689,  Val_Acc: 0.7232, Val_F1: 0.4594, Val_Pre: 0.7020, Val_Rec: 0.5177.
[2022-08-02 20:05:15,696 main.py]: Epoch [47/50],  epoch_train_loss: 0.3429, epoch_val_loss: 0.5330, Train_Acc: 0.9700,  Val_Acc: 0.7681, Val_F1: 0.6209, Val_Pre: 0.7713, Val_Rec: 0.6132.
[2022-08-02 20:06:52,458 main.py]: Epoch [48/50],  epoch_train_loss: 0.3421, epoch_val_loss: 0.5696, Train_Acc: 0.9708,  Val_Acc: 0.7181, Val_F1: 0.4506, Val_Pre: 0.6300, Val_Rec: 0.5118.
[2022-08-02 20:08:29,768 main.py]: Epoch [49/50],  epoch_train_loss: 0.3425, epoch_val_loss: 0.5457, Train_Acc: 0.9703,  Val_Acc: 0.7572, Val_F1: 0.6072, Val_Pre: 0.7344, Val_Rec: 0.6025.
[2022-08-02 20:10:06,102 main.py]: Epoch [50/50],  epoch_train_loss: 0.3432, epoch_val_loss: 0.5741, Train_Acc: 0.9699,  Val_Acc: 0.7145, Val_F1: 0.4732, Val_Pre: 0.5986, Val_Rec: 0.5186.
[2022-08-02 20:10:06,103 main.py]: start testing model
[2022-08-02 20:10:06,744 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 20:10:06,745 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 20:10:06,872 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-02 20:10:15,442 main.py]: showing the cache model metrics
[2022-08-02 20:10:15,442 main.py]: Classification Acc: 0.9217, F1: 0.9064, Precision: 0.8962, Recall: 0.9191, AUC-ROC: 0.9691
[2022-08-02 20:10:15,445 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.96      0.93      0.94       989
           1       0.83      0.91      0.87       391
    accuracy                           0.92      1380
   macro avg       0.90      0.92      0.91      1380
weighted avg       0.93      0.92      0.92      1380
[2022-08-02 20:10:15,445 main.py]: Classification confusion matrix:
[[915  74]
 [ 34 357]]