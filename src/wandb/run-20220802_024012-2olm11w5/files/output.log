[2022-08-02 02:40:14,275 main.py]: loading data
[2022-08-02 02:40:14,945 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-08-02 02:40:14,945 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
[2022-08-02 02:40:26,946 main.py]: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-08-02 02:40:27,009 main.py]: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 02:40:27,009 main.py]: building model
[2022-08-02 02:40:27,643 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 02:40:27,643 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 02:40:27,829 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-02 02:40:30,158 main.py]: expType: vis, train in expCode: vis_epoch50_freeze, batch_size 256, epoch = 50, lr = 0.0001
[2022-08-02 02:40:31,043 main.py]: start training model
[2022-08-02 02:40:39,949 main.py]: Epoch [1/50],  epoch_train_loss: 0.5456, epoch_val_loss: 0.4873, Train_Acc: 0.8255,  Val_Acc: 0.8761, Val_F1: 0.8211, Val_Pre: 0.9224, Val_Rec: 0.7829.
[2022-08-02 02:40:48,384 main.py]: Epoch [2/50],  epoch_train_loss: 0.4363, epoch_val_loss: 0.4469, Train_Acc: 0.9023,  Val_Acc: 0.8862, Val_F1: 0.8384, Val_Pre: 0.9279, Val_Rec: 0.8008.
[2022-08-02 02:40:57,141 main.py]: Epoch [3/50],  epoch_train_loss: 0.4110, epoch_val_loss: 0.4460, Train_Acc: 0.9196,  Val_Acc: 0.8746, Val_F1: 0.8186, Val_Pre: 0.9217, Val_Rec: 0.7803.
[2022-08-02 02:41:04,842 main.py]: Epoch [4/50],  epoch_train_loss: 0.3958, epoch_val_loss: 0.4528, Train_Acc: 0.9309,  Val_Acc: 0.7572, Val_F1: 0.5915, Val_Pre: 0.7560, Val_Rec: 0.5925.
[2022-08-02 02:41:12,745 main.py]: Epoch [5/50],  epoch_train_loss: 0.3871, epoch_val_loss: 0.4604, Train_Acc: 0.9383,  Val_Acc: 0.7587, Val_F1: 0.5952, Val_Pre: 0.7588, Val_Rec: 0.5950.
[2022-08-02 02:41:20,494 main.py]: Epoch [6/50],  epoch_train_loss: 0.3810, epoch_val_loss: 0.4590, Train_Acc: 0.9420,  Val_Acc: 0.7565, Val_F1: 0.5897, Val_Pre: 0.7546, Val_Rec: 0.5912.
[2022-08-02 02:41:28,383 main.py]: Epoch [7/50],  epoch_train_loss: 0.3767, epoch_val_loss: 0.4625, Train_Acc: 0.9442,  Val_Acc: 0.7551, Val_F1: 0.5885, Val_Pre: 0.7476, Val_Rec: 0.5902.
[2022-08-02 02:41:36,119 main.py]: Epoch [8/50],  epoch_train_loss: 0.3747, epoch_val_loss: 0.4823, Train_Acc: 0.9455,  Val_Acc: 0.7565, Val_F1: 0.5897, Val_Pre: 0.7546, Val_Rec: 0.5912.
[2022-08-02 02:41:43,501 main.py]: Epoch [9/50],  epoch_train_loss: 0.3715, epoch_val_loss: 0.4886, Train_Acc: 0.9468,  Val_Acc: 0.7536, Val_F1: 0.5861, Val_Pre: 0.7427, Val_Rec: 0.5884.
[2022-08-02 02:41:51,003 main.py]: Epoch [10/50],  epoch_train_loss: 0.3702, epoch_val_loss: 0.4881, Train_Acc: 0.9480,  Val_Acc: 0.7536, Val_F1: 0.5861, Val_Pre: 0.7427, Val_Rec: 0.5884.
[2022-08-02 02:41:58,646 main.py]: Epoch [11/50],  epoch_train_loss: 0.3688, epoch_val_loss: 0.4919, Train_Acc: 0.9488,  Val_Acc: 0.7536, Val_F1: 0.5861, Val_Pre: 0.7427, Val_Rec: 0.5884.
[2022-08-02 02:42:05,641 main.py]: Epoch [12/50],  epoch_train_loss: 0.3674, epoch_val_loss: 0.4845, Train_Acc: 0.9498,  Val_Acc: 0.7558, Val_F1: 0.5916, Val_Pre: 0.7472, Val_Rec: 0.5923.
[2022-08-02 02:42:13,153 main.py]: Epoch [13/50],  epoch_train_loss: 0.3674, epoch_val_loss: 0.4871, Train_Acc: 0.9500,  Val_Acc: 0.7565, Val_F1: 0.5935, Val_Pre: 0.7486, Val_Rec: 0.5935.
[2022-08-02 02:42:20,482 main.py]: Epoch [14/50],  epoch_train_loss: 0.3667, epoch_val_loss: 0.4902, Train_Acc: 0.9502,  Val_Acc: 0.7558, Val_F1: 0.5916, Val_Pre: 0.7472, Val_Rec: 0.5923.
[2022-08-02 02:42:27,479 main.py]: Epoch [15/50],  epoch_train_loss: 0.3662, epoch_val_loss: 0.4782, Train_Acc: 0.9504,  Val_Acc: 0.7587, Val_F1: 0.5989, Val_Pre: 0.7528, Val_Rec: 0.5974.
[2022-08-02 02:42:34,831 main.py]: Epoch [16/50],  epoch_train_loss: 0.3650, epoch_val_loss: 0.4954, Train_Acc: 0.9508,  Val_Acc: 0.7543, Val_F1: 0.5879, Val_Pre: 0.7442, Val_Rec: 0.5897.
[2022-08-02 02:42:41,744 main.py]: Epoch [17/50],  epoch_train_loss: 0.3656, epoch_val_loss: 0.4949, Train_Acc: 0.9505,  Val_Acc: 0.7558, Val_F1: 0.5916, Val_Pre: 0.7472, Val_Rec: 0.5923.
[2022-08-02 02:42:49,153 main.py]: Epoch [18/50],  epoch_train_loss: 0.3651, epoch_val_loss: 0.4989, Train_Acc: 0.9510,  Val_Acc: 0.7674, Val_F1: 0.6203, Val_Pre: 0.7682, Val_Rec: 0.6127.
[2022-08-02 02:42:56,139 main.py]: Epoch [19/50],  epoch_train_loss: 0.3640, epoch_val_loss: 0.5027, Train_Acc: 0.9510,  Val_Acc: 0.7688, Val_F1: 0.6237, Val_Pre: 0.7706, Val_Rec: 0.6153.
[2022-08-02 02:43:03,615 main.py]: Epoch [20/50],  epoch_train_loss: 0.3642, epoch_val_loss: 0.4969, Train_Acc: 0.9510,  Val_Acc: 0.7659, Val_F1: 0.6168, Val_Pre: 0.7658, Val_Rec: 0.6102.
[2022-08-02 02:43:11,154 main.py]: Epoch [21/50],  epoch_train_loss: 0.3639, epoch_val_loss: 0.4874, Train_Acc: 0.9511,  Val_Acc: 0.7710, Val_F1: 0.6289, Val_Pre: 0.7740, Val_Rec: 0.6191.
[2022-08-02 02:43:18,180 main.py]: Epoch [22/50],  epoch_train_loss: 0.3633, epoch_val_loss: 0.4959, Train_Acc: 0.9510,  Val_Acc: 0.7674, Val_F1: 0.6180, Val_Pre: 0.7721, Val_Rec: 0.6112.
[2022-08-02 02:43:25,664 main.py]: Epoch [23/50],  epoch_train_loss: 0.3631, epoch_val_loss: 0.4915, Train_Acc: 0.9512,  Val_Acc: 0.7674, Val_F1: 0.6180, Val_Pre: 0.7721, Val_Rec: 0.6112.
[2022-08-02 02:43:32,628 main.py]: Epoch [24/50],  epoch_train_loss: 0.3623, epoch_val_loss: 0.4797, Train_Acc: 0.9512,  Val_Acc: 0.7674, Val_F1: 0.6180, Val_Pre: 0.7721, Val_Rec: 0.6112.
[2022-08-02 02:43:40,093 main.py]: Epoch [25/50],  epoch_train_loss: 0.3633, epoch_val_loss: 0.4774, Train_Acc: 0.9519,  Val_Acc: 0.7674, Val_F1: 0.6180, Val_Pre: 0.7721, Val_Rec: 0.6112.
[2022-08-02 02:43:47,154 main.py]: Epoch [26/50],  epoch_train_loss: 0.3626, epoch_val_loss: 0.4634, Train_Acc: 0.9520,  Val_Acc: 0.7746, Val_F1: 0.6353, Val_Pre: 0.7833, Val_Rec: 0.6240.
[2022-08-02 02:43:54,613 main.py]: Epoch [27/50],  epoch_train_loss: 0.3625, epoch_val_loss: 0.4687, Train_Acc: 0.9522,  Val_Acc: 0.7725, Val_F1: 0.6302, Val_Pre: 0.7801, Val_Rec: 0.6201.
[2022-08-02 02:44:01,589 main.py]: Epoch [28/50],  epoch_train_loss: 0.3621, epoch_val_loss: 0.4620, Train_Acc: 0.9522,  Val_Acc: 0.7710, Val_F1: 0.6267, Val_Pre: 0.7778, Val_Rec: 0.6176.
[2022-08-02 02:44:09,070 main.py]: Epoch [29/50],  epoch_train_loss: 0.3618, epoch_val_loss: 0.4686, Train_Acc: 0.9521,  Val_Acc: 0.7870, Val_F1: 0.6341, Val_Pre: 0.8854, Val_Rec: 0.6240.
[2022-08-02 02:44:16,025 main.py]: Epoch [30/50],  epoch_train_loss: 0.3623, epoch_val_loss: 0.4686, Train_Acc: 0.9524,  Val_Acc: 0.7710, Val_F1: 0.6256, Val_Pre: 0.7798, Val_Rec: 0.6168.
[2022-08-02 02:44:23,378 main.py]: Epoch [31/50],  epoch_train_loss: 0.3622, epoch_val_loss: 0.4503, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-08-02 02:44:30,262 main.py]: Epoch [32/50],  epoch_train_loss: 0.3619, epoch_val_loss: 0.4478, Train_Acc: 0.9523,  Val_Acc: 0.7746, Val_F1: 0.6353, Val_Pre: 0.7833, Val_Rec: 0.6240.
[2022-08-02 02:44:37,592 main.py]: Epoch [33/50],  epoch_train_loss: 0.3615, epoch_val_loss: 0.4532, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-08-02 02:44:44,564 main.py]: Epoch [34/50],  epoch_train_loss: 0.3622, epoch_val_loss: 0.4588, Train_Acc: 0.9523,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-08-02 02:44:52,057 main.py]: Epoch [35/50],  epoch_train_loss: 0.3620, epoch_val_loss: 0.4439, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-08-02 02:44:59,041 main.py]: Epoch [36/50],  epoch_train_loss: 0.3611, epoch_val_loss: 0.4553, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-08-02 02:45:06,443 main.py]: Epoch [37/50],  epoch_train_loss: 0.3619, epoch_val_loss: 0.4530, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-08-02 02:45:13,408 main.py]: Epoch [38/50],  epoch_train_loss: 0.3614, epoch_val_loss: 0.4405, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-08-02 02:45:20,716 main.py]: Epoch [39/50],  epoch_train_loss: 0.3606, epoch_val_loss: 0.4462, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-08-02 02:45:27,703 main.py]: Epoch [40/50],  epoch_train_loss: 0.3618, epoch_val_loss: 0.4242, Train_Acc: 0.9524,  Val_Acc: 0.8732, Val_F1: 0.8263, Val_Pre: 0.8822, Val_Rec: 0.7979.
[2022-08-02 02:45:35,033 main.py]: Epoch [41/50],  epoch_train_loss: 0.3621, epoch_val_loss: 0.4388, Train_Acc: 0.9524,  Val_Acc: 0.7696, Val_F1: 0.6221, Val_Pre: 0.7776, Val_Rec: 0.6142.
[2022-08-02 02:45:41,992 main.py]: Epoch [42/50],  epoch_train_loss: 0.3627, epoch_val_loss: 0.4256, Train_Acc: 0.9524,  Val_Acc: 0.8732, Val_F1: 0.8259, Val_Pre: 0.8833, Val_Rec: 0.7971.
[2022-08-02 02:45:49,352 main.py]: Epoch [43/50],  epoch_train_loss: 0.3614, epoch_val_loss: 0.4719, Train_Acc: 0.9524,  Val_Acc: 0.7645, Val_F1: 0.6097, Val_Pre: 0.7692, Val_Rec: 0.6053.
[2022-08-02 02:45:56,402 main.py]: Epoch [44/50],  epoch_train_loss: 0.3607, epoch_val_loss: 0.4447, Train_Acc: 0.9525,  Val_Acc: 0.7674, Val_F1: 0.6169, Val_Pre: 0.7741, Val_Rec: 0.6104.
[2022-08-02 02:46:03,823 main.py]: Epoch [45/50],  epoch_train_loss: 0.3613, epoch_val_loss: 0.4299, Train_Acc: 0.9525,  Val_Acc: 0.7870, Val_F1: 0.6341, Val_Pre: 0.8854, Val_Rec: 0.6240.
[2022-08-02 02:46:10,756 main.py]: Epoch [46/50],  epoch_train_loss: 0.3603, epoch_val_loss: 0.4487, Train_Acc: 0.9524,  Val_Acc: 0.7717, Val_F1: 0.6274, Val_Pre: 0.7810, Val_Rec: 0.6181.
[2022-08-02 02:46:18,203 main.py]: Epoch [47/50],  epoch_train_loss: 0.3604, epoch_val_loss: 0.4381, Train_Acc: 0.9524,  Val_Acc: 0.7717, Val_F1: 0.6274, Val_Pre: 0.7810, Val_Rec: 0.6181.
[2022-08-02 02:46:25,750 main.py]: Epoch [48/50],  epoch_train_loss: 0.3606, epoch_val_loss: 0.4040, Train_Acc: 0.9525,  Val_Acc: 0.8732, Val_F1: 0.8259, Val_Pre: 0.8833, Val_Rec: 0.7971.
[2022-08-02 02:46:32,665 main.py]: Epoch [49/50],  epoch_train_loss: 0.3615, epoch_val_loss: 0.4196, Train_Acc: 0.9525,  Val_Acc: 0.8732, Val_F1: 0.8259, Val_Pre: 0.8833, Val_Rec: 0.7971.
[2022-08-02 02:46:39,647 main.py]: Epoch [50/50],  epoch_train_loss: 0.3611, epoch_val_loss: 0.4341, Train_Acc: 0.9523,  Val_Acc: 0.7891, Val_F1: 0.6395, Val_Pre: 0.8863, Val_Rec: 0.6279.
[2022-08-02 02:46:39,648 main.py]: start testing model
[2022-08-02 02:46:40,312 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 02:46:40,312 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 02:46:40,439 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-02 02:46:43,313 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-02 02:46:43,314 configuration_utils.py]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-02 02:46:43,351 modeling_utils.py]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-02 02:46:46,914 main.py]: Classification Acc: 0.8862, F1: 0.8384, Precision: 0.9279, Recall: 0.8008, AUC-ROC: 0.9665
[2022-08-02 02:46:46,917 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.86      1.00      0.93       989
           1       0.99      0.60      0.75       391
    accuracy                           0.89      1380
   macro avg       0.93      0.80      0.84      1380
weighted avg       0.90      0.89      0.88      1380
[2022-08-02 02:46:46,918 main.py]: Classification confusion matrix:
[[987   2]
 [155 236]]
[2022-08-02 02:46:47,532 main.py]: Classification Acc: 0.8862, F1: 0.8384, Precision: 0.9279, Recall: 0.8008, AUC-ROC: 0.9665
[2022-08-02 02:46:47,536 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.86      1.00      0.93       989
           1       0.99      0.60      0.75       391
    accuracy                           0.89      1380
   macro avg       0.93      0.80      0.84      1380
weighted avg       0.90      0.89      0.88      1380
[2022-08-02 02:46:47,536 main.py]: Classification confusion matrix:
[[987   2]
 [155 236]]
--------------------------------------------------------------------------------