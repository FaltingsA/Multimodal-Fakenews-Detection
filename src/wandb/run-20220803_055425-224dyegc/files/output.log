[2022-08-03 05:54:27,308 main.py]: loading data
[2022-08-03 05:54:27,308 tokenization_utils.py]: Model name 'hfl/chinese-roberta-wwm-ext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'hfl/chinese-roberta-wwm-ext' is a path, a model identifier, or url to a directory containing tokenizer files.
[2022-08-03 05:54:28,648 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/vocab.txt from cache at /home/v-zuangao/.cache/torch/transformers/5593eb652e3fb9a17042385245a61389ce6f0c8a25e167519477d7efbdf2459a.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00
[2022-08-03 05:54:28,648 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/added_tokens.json from cache at /home/v-zuangao/.cache/torch/transformers/23740a16768d945f44a24590dc8f5e572773b1b2868c5e58f7ff4fae2a721c49.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2
[2022-08-03 05:54:28,648 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/special_tokens_map.json from cache at /home/v-zuangao/.cache/torch/transformers/6f13f9fe28f96dd7be36b84708332115ef90b3b310918502c13a8f719a225de2.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4
[2022-08-03 05:54:28,648 tokenization_utils.py]: loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/tokenizer_config.json from cache at /home/v-zuangao/.cache/torch/transformers/5bb5761fdb6c8f42bf7705c27c48cffd8b40afa8278fa035bc81bf288f108af9.1ade4e0ac224a06d83f2cb9821a6656b6b59974d6552e8c728f2657e4ba445d9
Building prefix dict from the default dictionary ...
[2022-08-03 05:56:32,250 __init__.py]: Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
[2022-08-03 05:56:32,250 __init__.py]: Loading model from cache /tmp/jieba.cache
Loading model cost 0.467 seconds.
[2022-08-03 05:56:32,718 __init__.py]: Loading model cost 0.467 seconds.
Prefix dict has been built successfully.
[2022-08-03 05:56:32,718 __init__.py]: Prefix dict has been built successfully.
TEXT: 4766, Image: 4766, label: 4766, Mask: 4766
TEXT: 1331, Image: 1331, label: 1331, Mask: 1331
[2022-08-03 05:58:53,414 main.py]: Train data_length => TEXT: 4766, Image: 4766, label: 4766, Mask: 4766
[2022-08-03 05:58:53,470 main.py]: Test data_length => TEXT: 1331, Image: 1331, label: 1331, Mask: 1331
[2022-08-03 05:58:53,470 main.py]: building model
[2022-08-03 05:58:54,049 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/config.json from cache at /home/v-zuangao/.cache/torch/transformers/3227a4c62d38cb0f891e7ccb8fc0b7b7dbd873889b3688c2e95834fb6f6109be.573c56a39f56f50b7bc6e9803d92871c74059419e37e87c5b14d49b1546b0703
[2022-08-03 05:58:54,049 configuration_utils.py]: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
[2022-08-03 05:58:54,144 modeling_utils.py]: loading weights file https://cdn.huggingface.co/hfl/chinese-roberta-wwm-ext/pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/47d2326d47246cef3121d70d592c0391a4ed594b04ce3dea8bd47edd37e20370.6ac27309c356295f0e005c6029fce503ec6a32853911ebf79f8bddd8dd10edad
[2022-08-03 05:58:55,317 main.py]: expType: mulT, train in expCode: mulT_epoch50_freeze2, batch_size 256, epoch = 50, lr = 0.0001
[2022-08-03 05:58:56,121 main.py]: start training model
Checkpoint Directory exists!
[2022-08-03 05:59:27,779 main.py]: Epoch [1/50],  epoch_train_loss: 0.6557, epoch_val_loss: 0.6226, Train_Acc: 0.6217,  Val_Acc: 0.6582, Val_F1: 0.6533, Val_Pre: 0.6626, Val_Rec: 0.6555.
Checkpoint Directory exists!
[2022-08-03 06:00:00,588 main.py]: Epoch [2/50],  epoch_train_loss: 0.5832, epoch_val_loss: 0.5406, Train_Acc: 0.7398,  Val_Acc: 0.7641, Val_F1: 0.7641, Val_Pre: 0.7642, Val_Rec: 0.7644.
Checkpoint Directory exists!
[2022-08-03 06:00:46,690 main.py]: Epoch [3/50],  epoch_train_loss: 0.5024, epoch_val_loss: 0.5092, Train_Acc: 0.8170,  Val_Acc: 0.8009, Val_F1: 0.7995, Val_Pre: 0.8150, Val_Rec: 0.8036.
Checkpoint Directory exists!
[2022-08-03 06:01:31,859 main.py]: Epoch [4/50],  epoch_train_loss: 0.4519, epoch_val_loss: 0.4974, Train_Acc: 0.8594,  Val_Acc: 0.8182, Val_F1: 0.8175, Val_Pre: 0.8270, Val_Rec: 0.8203.
[2022-08-03 06:02:17,508 main.py]: Epoch [5/50],  epoch_train_loss: 0.4289, epoch_val_loss: 0.5159, Train_Acc: 0.8817,  Val_Acc: 0.8069, Val_F1: 0.8036, Val_Pre: 0.8381, Val_Rec: 0.8109.
Checkpoint Directory exists!
[2022-08-03 06:02:49,236 main.py]: Epoch [6/50],  epoch_train_loss: 0.4190, epoch_val_loss: 0.4992, Train_Acc: 0.8907,  Val_Acc: 0.8242, Val_F1: 0.8224, Val_Pre: 0.8453, Val_Rec: 0.8274.
[2022-08-03 06:03:34,689 main.py]: Epoch [7/50],  epoch_train_loss: 0.4059, epoch_val_loss: 0.5340, Train_Acc: 0.9064,  Val_Acc: 0.7911, Val_F1: 0.7858, Val_Pre: 0.8352, Val_Rec: 0.7959.
[2022-08-03 06:04:06,521 main.py]: Epoch [8/50],  epoch_train_loss: 0.3997, epoch_val_loss: 0.5551, Train_Acc: 0.9129,  Val_Acc: 0.7799, Val_F1: 0.7725, Val_Pre: 0.8351, Val_Rec: 0.7851.
[2022-08-03 06:04:38,425 main.py]: Epoch [9/50],  epoch_train_loss: 0.3879, epoch_val_loss: 0.5105, Train_Acc: 0.9255,  Val_Acc: 0.8197, Val_F1: 0.8172, Val_Pre: 0.8464, Val_Rec: 0.8233.
[2022-08-03 06:05:10,424 main.py]: Epoch [10/50],  epoch_train_loss: 0.3811, epoch_val_loss: 0.5800, Train_Acc: 0.9310,  Val_Acc: 0.7468, Val_F1: 0.7346, Val_Pre: 0.8192, Val_Rec: 0.7530.
[2022-08-03 06:05:42,317 main.py]: Epoch [11/50],  epoch_train_loss: 0.3832, epoch_val_loss: 0.5096, Train_Acc: 0.9272,  Val_Acc: 0.8204, Val_F1: 0.8180, Val_Pre: 0.8469, Val_Rec: 0.8240.
[2022-08-03 06:06:14,284 main.py]: Epoch [12/50],  epoch_train_loss: 0.3774, epoch_val_loss: 0.5395, Train_Acc: 0.9356,  Val_Acc: 0.7926, Val_F1: 0.7873, Val_Pre: 0.8369, Val_Rec: 0.7974.
Checkpoint Directory exists!
[2022-08-03 06:06:46,249 main.py]: Epoch [13/50],  epoch_train_loss: 0.3767, epoch_val_loss: 0.4943, Train_Acc: 0.9362,  Val_Acc: 0.8264, Val_F1: 0.8243, Val_Pre: 0.8515, Val_Rec: 0.8299.
Checkpoint Directory exists!
[2022-08-03 06:07:31,792 main.py]: Epoch [14/50],  epoch_train_loss: 0.3668, epoch_val_loss: 0.4697, Train_Acc: 0.9482,  Val_Acc: 0.8520, Val_F1: 0.8514, Val_Pre: 0.8624, Val_Rec: 0.8542.
[2022-08-03 06:08:17,637 main.py]: Epoch [15/50],  epoch_train_loss: 0.3682, epoch_val_loss: 0.4917, Train_Acc: 0.9440,  Val_Acc: 0.8279, Val_F1: 0.8259, Val_Pre: 0.8525, Val_Rec: 0.8314.
[2022-08-03 06:08:49,482 main.py]: Epoch [16/50],  epoch_train_loss: 0.3620, epoch_val_loss: 0.5658, Train_Acc: 0.9515,  Val_Acc: 0.7641, Val_F1: 0.7552, Val_Pre: 0.8228, Val_Rec: 0.7696.
[2022-08-03 06:09:21,514 main.py]: Epoch [17/50],  epoch_train_loss: 0.3657, epoch_val_loss: 0.4754, Train_Acc: 0.9457,  Val_Acc: 0.8490, Val_F1: 0.8482, Val_Pre: 0.8613, Val_Rec: 0.8514.
[2022-08-03 06:09:53,487 main.py]: Epoch [18/50],  epoch_train_loss: 0.3654, epoch_val_loss: 0.5546, Train_Acc: 0.9482,  Val_Acc: 0.7716, Val_F1: 0.7633, Val_Pre: 0.8297, Val_Rec: 0.7771.
[2022-08-03 06:10:25,469 main.py]: Epoch [19/50],  epoch_train_loss: 0.3597, epoch_val_loss: 0.5192, Train_Acc: 0.9549,  Val_Acc: 0.8047, Val_F1: 0.8006, Val_Pre: 0.8424, Val_Rec: 0.8090.
[2022-08-03 06:10:57,420 main.py]: Epoch [20/50],  epoch_train_loss: 0.3554, epoch_val_loss: 0.5757, Train_Acc: 0.9585,  Val_Acc: 0.7596, Val_F1: 0.7500, Val_Pre: 0.8214, Val_Rec: 0.7653.
[2022-08-03 06:11:29,388 main.py]: Epoch [21/50],  epoch_train_loss: 0.3608, epoch_val_loss: 0.4844, Train_Acc: 0.9536,  Val_Acc: 0.8422, Val_F1: 0.8411, Val_Pre: 0.8586, Val_Rec: 0.8450.
[2022-08-03 06:12:01,489 main.py]: Epoch [22/50],  epoch_train_loss: 0.3622, epoch_val_loss: 0.4925, Train_Acc: 0.9511,  Val_Acc: 0.8325, Val_F1: 0.8307, Val_Pre: 0.8543, Val_Rec: 0.8357.
[2022-08-03 06:12:33,570 main.py]: Epoch [23/50],  epoch_train_loss: 0.3528, epoch_val_loss: 0.5109, Train_Acc: 0.9603,  Val_Acc: 0.8159, Val_F1: 0.8131, Val_Pre: 0.8452, Val_Rec: 0.8197.
[2022-08-03 06:13:05,609 main.py]: Epoch [24/50],  epoch_train_loss: 0.3561, epoch_val_loss: 0.4957, Train_Acc: 0.9582,  Val_Acc: 0.8332, Val_F1: 0.8314, Val_Pre: 0.8554, Val_Rec: 0.8365.
[2022-08-03 06:13:37,519 main.py]: Epoch [25/50],  epoch_train_loss: 0.3515, epoch_val_loss: 0.4874, Train_Acc: 0.9622,  Val_Acc: 0.8377, Val_F1: 0.8365, Val_Pre: 0.8544, Val_Rec: 0.8406.
[2022-08-03 06:14:09,477 main.py]: Epoch [26/50],  epoch_train_loss: 0.3485, epoch_val_loss: 0.4922, Train_Acc: 0.9658,  Val_Acc: 0.8317, Val_F1: 0.8300, Val_Pre: 0.8527, Val_Rec: 0.8349.
[2022-08-03 06:14:41,544 main.py]: Epoch [27/50],  epoch_train_loss: 0.3503, epoch_val_loss: 0.4730, Train_Acc: 0.9645,  Val_Acc: 0.8475, Val_F1: 0.8470, Val_Pre: 0.8567, Val_Rec: 0.8496.
[2022-08-03 06:15:13,704 main.py]: Epoch [28/50],  epoch_train_loss: 0.3508, epoch_val_loss: 0.5280, Train_Acc: 0.9614,  Val_Acc: 0.8002, Val_F1: 0.7957, Val_Pre: 0.8397, Val_Rec: 0.8046.
[2022-08-03 06:15:45,694 main.py]: Epoch [29/50],  epoch_train_loss: 0.3485, epoch_val_loss: 0.5156, Train_Acc: 0.9666,  Val_Acc: 0.8129, Val_F1: 0.8097, Val_Pre: 0.8446, Val_Rec: 0.8169.
[2022-08-03 06:16:17,669 main.py]: Epoch [30/50],  epoch_train_loss: 0.3460, epoch_val_loss: 0.5583, Train_Acc: 0.9687,  Val_Acc: 0.7724, Val_F1: 0.7651, Val_Pre: 0.8235, Val_Rec: 0.7775.
[2022-08-03 06:16:49,698 main.py]: Epoch [31/50],  epoch_train_loss: 0.3450, epoch_val_loss: 0.4982, Train_Acc: 0.9692,  Val_Acc: 0.8264, Val_F1: 0.8243, Val_Pre: 0.8515, Val_Rec: 0.8299.
[2022-08-03 06:17:21,697 main.py]: Epoch [32/50],  epoch_train_loss: 0.3447, epoch_val_loss: 0.4894, Train_Acc: 0.9694,  Val_Acc: 0.8355, Val_F1: 0.8335, Val_Pre: 0.8598, Val_Rec: 0.8389.
[2022-08-03 06:17:53,750 main.py]: Epoch [33/50],  epoch_train_loss: 0.3520, epoch_val_loss: 0.5759, Train_Acc: 0.9614,  Val_Acc: 0.7618, Val_F1: 0.7528, Val_Pre: 0.8205, Val_Rec: 0.7674.
[2022-08-03 06:18:25,680 main.py]: Epoch [34/50],  epoch_train_loss: 0.3461, epoch_val_loss: 0.5229, Train_Acc: 0.9666,  Val_Acc: 0.8039, Val_F1: 0.7999, Val_Pre: 0.8404, Val_Rec: 0.8082.
[2022-08-03 06:18:57,589 main.py]: Epoch [35/50],  epoch_train_loss: 0.3403, epoch_val_loss: 0.4985, Train_Acc: 0.9736,  Val_Acc: 0.8295, Val_F1: 0.8275, Val_Pre: 0.8529, Val_Rec: 0.8328.
[2022-08-03 06:19:29,489 main.py]: Epoch [36/50],  epoch_train_loss: 0.3420, epoch_val_loss: 0.5105, Train_Acc: 0.9721,  Val_Acc: 0.8159, Val_F1: 0.8134, Val_Pre: 0.8420, Val_Rec: 0.8195.
[2022-08-03 06:20:01,356 main.py]: Epoch [37/50],  epoch_train_loss: 0.3394, epoch_val_loss: 0.5299, Train_Acc: 0.9740,  Val_Acc: 0.7979, Val_F1: 0.7936, Val_Pre: 0.8353, Val_Rec: 0.8022.
[2022-08-03 06:20:33,174 main.py]: Epoch [38/50],  epoch_train_loss: 0.3392, epoch_val_loss: 0.4944, Train_Acc: 0.9748,  Val_Acc: 0.8264, Val_F1: 0.8248, Val_Pre: 0.8459, Val_Rec: 0.8295.
[2022-08-03 06:21:04,979 main.py]: Epoch [39/50],  epoch_train_loss: 0.3392, epoch_val_loss: 0.4876, Train_Acc: 0.9748,  Val_Acc: 0.8325, Val_F1: 0.8307, Val_Pre: 0.8538, Val_Rec: 0.8357.
[2022-08-03 06:21:36,916 main.py]: Epoch [40/50],  epoch_train_loss: 0.3380, epoch_val_loss: 0.5114, Train_Acc: 0.9763,  Val_Acc: 0.8107, Val_F1: 0.8079, Val_Pre: 0.8386, Val_Rec: 0.8144.
[2022-08-03 06:22:08,838 main.py]: Epoch [41/50],  epoch_train_loss: 0.3360, epoch_val_loss: 0.5247, Train_Acc: 0.9782,  Val_Acc: 0.8032, Val_F1: 0.7993, Val_Pre: 0.8385, Val_Rec: 0.8074.
[2022-08-03 06:22:40,875 main.py]: Epoch [42/50],  epoch_train_loss: 0.3365, epoch_val_loss: 0.5234, Train_Acc: 0.9775,  Val_Acc: 0.8009, Val_F1: 0.7969, Val_Pre: 0.8364, Val_Rec: 0.8051.
[2022-08-03 06:23:12,792 main.py]: Epoch [43/50],  epoch_train_loss: 0.3374, epoch_val_loss: 0.5184, Train_Acc: 0.9769,  Val_Acc: 0.8092, Val_F1: 0.8060, Val_Pre: 0.8395, Val_Rec: 0.8131.
[2022-08-03 06:23:44,747 main.py]: Epoch [44/50],  epoch_train_loss: 0.3364, epoch_val_loss: 0.5548, Train_Acc: 0.9769,  Val_Acc: 0.7739, Val_F1: 0.7670, Val_Pre: 0.8226, Val_Rec: 0.7789.
[2022-08-03 06:24:16,704 main.py]: Epoch [45/50],  epoch_train_loss: 0.3403, epoch_val_loss: 0.4740, Train_Acc: 0.9727,  Val_Acc: 0.8430, Val_F1: 0.8424, Val_Pre: 0.8521, Val_Rec: 0.8451.
[2022-08-03 06:24:48,673 main.py]: Epoch [46/50],  epoch_train_loss: 0.3389, epoch_val_loss: 0.4839, Train_Acc: 0.9748,  Val_Acc: 0.8370, Val_F1: 0.8361, Val_Pre: 0.8498, Val_Rec: 0.8395.
[2022-08-03 06:25:20,593 main.py]: Epoch [47/50],  epoch_train_loss: 0.3349, epoch_val_loss: 0.5077, Train_Acc: 0.9792,  Val_Acc: 0.8159, Val_F1: 0.8132, Val_Pre: 0.8445, Val_Rec: 0.8197.
[2022-08-03 06:25:52,454 main.py]: Epoch [48/50],  epoch_train_loss: 0.3337, epoch_val_loss: 0.4718, Train_Acc: 0.9796,  Val_Acc: 0.8512, Val_F1: 0.8506, Val_Pre: 0.8618, Val_Rec: 0.8535.
[2022-08-03 06:26:24,290 main.py]: Epoch [49/50],  epoch_train_loss: 0.3372, epoch_val_loss: 0.4874, Train_Acc: 0.9780,  Val_Acc: 0.8325, Val_F1: 0.8312, Val_Pre: 0.8491, Val_Rec: 0.8353.
[2022-08-03 06:26:56,220 main.py]: Epoch [50/50],  epoch_train_loss: 0.3385, epoch_val_loss: 0.4796, Train_Acc: 0.9752,  Val_Acc: 0.8392, Val_F1: 0.8388, Val_Pre: 0.8471, Val_Rec: 0.8412.
[2022-08-03 06:26:56,220 main.py]: start testing model
[2022-08-03 06:26:56,904 configuration_utils.py]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-roberta-wwm-ext/config.json from cache at /home/v-zuangao/.cache/torch/transformers/3227a4c62d38cb0f891e7ccb8fc0b7b7dbd873889b3688c2e95834fb6f6109be.573c56a39f56f50b7bc6e9803d92871c74059419e37e87c5b14d49b1546b0703
[2022-08-03 06:26:56,904 configuration_utils.py]: Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "directionality": "bidi",
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
[2022-08-03 06:26:57,011 modeling_utils.py]: loading weights file https://cdn.huggingface.co/hfl/chinese-roberta-wwm-ext/pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/47d2326d47246cef3121d70d592c0391a4ed594b04ce3dea8bd47edd37e20370.6ac27309c356295f0e005c6029fce503ec6a32853911ebf79f8bddd8dd10edad
[2022-08-03 06:27:04,341 main.py]: showing the cache model metrics
[2022-08-03 06:27:04,342 main.py]: Classification Acc: 0.8520, F1: 0.8514, Precision: 0.8624, Recall: 0.8542, AUC-ROC: 0.9357
[2022-08-03 06:27:04,345 main.py]: Classification report:
              precision    recall  f1-score   support
           0       0.79      0.94      0.86       648
           1       0.93      0.77      0.84       683
    accuracy                           0.85      1331
   macro avg       0.86      0.85      0.85      1331
weighted avg       0.86      0.85      0.85      1331
[2022-08-03 06:27:04,345 main.py]: Classification confusion matrix:
[[609  39]
 [158 525]]