[2022-08-01 07:57:22,404 main.py: loading data
[2022-08-01 07:57:23,076 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/v-zuangao/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
[2022-08-01 07:57:23,077 tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/v-zuangao/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-01 07:57:48,397 main.py: Train data_length => TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
[2022-08-01 07:57:48,454 main.py: Test data_length => TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
[2022-08-01 07:57:48,455 main.py: building model
[2022-08-01 07:57:49,111 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-01 07:57:49,113 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-01 07:57:49,229 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-01 07:57:51,522 main.py: expType: mulT, train in expCode: mulT_debug, batch_size 256, epoch = 1, lr = 0.0001
[2022-08-01 07:57:52,349 main.py: start training model
[2022-08-01 07:59:27,021 main.py: Epoch [1/1],  epoch_train_loss: 0.4505, epoch_val_loss: 0.4558, Train_Acc: 0.8708,  Val_Acc: 0.8138, Val_F1: 0.6986, Val_Pre: 0.8934, Val_Rec: 0.6721.
[2022-08-01 07:59:27,810 main.py: start testing model
[2022-08-01 07:59:28,481 configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-08-01 07:59:28,482 configuration_utils.py: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-08-01 07:59:28,559 modeling_utils.py: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-08-01 07:59:37,186 main.py: Classification Acc: 0.8138, F1: 0.6986, Precision: 0.8934, Recall: 0.6721, AUC-ROC: 0.9444
[2022-08-01 07:59:37,192 main.py: Classification report:
              precision    recall  f1-score   support
           0       0.79      1.00      0.88       989
           1       0.99      0.35      0.51       391
    accuracy                           0.81      1380
   macro avg       0.89      0.67      0.70      1380
weighted avg       0.85      0.81      0.78      1380
[2022-08-01 07:59:37,193 main.py: Classification confusion matrix:
[[988   1]
 [256 135]]