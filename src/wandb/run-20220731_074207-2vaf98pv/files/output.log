
loading data
.DS_Store
image length 360
train data size is 14129
.DS_Store
image length 50
test data size is 1380
TEXT: 14129, Image: 14129, label: 14129, Mask: 14129
TEXT: 1380, Image: 1380, label: 1380, Mask: 1380
building model
expType: vis
start training model
[2022-07-31 07:42:57,729 main.py:178 INFO]: Epoch [1/10],  Loss: 0.5179, Class Loss: 0.5179, Train_Acc: 0.8144,  Validate_Acc: 0.8649.
[2022-07-31 07:43:22,616 main.py:178 INFO]: Epoch [2/10],  Loss: 0.4337, Class Loss: 0.4337, Train_Acc: 0.8985,  Validate_Acc: 0.7731.
[2022-07-31 07:43:34,407 main.py:178 INFO]: Epoch [3/10],  Loss: 0.4121, Class Loss: 0.4121, Train_Acc: 0.9162,  Validate_Acc: 0.8711.
[2022-07-31 07:43:57,083 main.py:178 INFO]: Epoch [4/10],  Loss: 0.4002, Class Loss: 0.4002, Train_Acc: 0.9277,  Validate_Acc: 0.8659.
[2022-07-31 07:44:08,855 main.py:178 INFO]: Epoch [5/10],  Loss: 0.3924, Class Loss: 0.3924, Train_Acc: 0.9322,  Validate_Acc: 0.8659.
[2022-07-31 07:44:20,296 main.py:178 INFO]: Epoch [6/10],  Loss: 0.3875, Class Loss: 0.3875, Train_Acc: 0.9356,  Validate_Acc: 0.8659.
[2022-07-31 07:44:32,195 main.py:178 INFO]: Epoch [7/10],  Loss: 0.3836, Class Loss: 0.3836, Train_Acc: 0.9377,  Validate_Acc: 0.8659.
[2022-07-31 07:44:43,665 main.py:178 INFO]: Epoch [8/10],  Loss: 0.3797, Class Loss: 0.3797, Train_Acc: 0.9420,  Validate_Acc: 0.8659.
[2022-07-31 07:44:55,553 main.py:178 INFO]: Epoch [9/10],  Loss: 0.3779, Class Loss: 0.3779, Train_Acc: 0.9422,  Validate_Acc: 0.8698.
[2022-07-31 07:45:07,114 main.py:178 INFO]: Epoch [10/10],  Loss: 0.3765, Class Loss: 0.3765, Train_Acc: 0.9431,  Validate_Acc: 0.8678.
[2022-07-31 07:45:07,116 main.py:189 INFO]: start testing model
[2022-07-31 07:45:08,235 configuration_utils.py:265 INFO]: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/v-zuangao/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
[2022-07-31 07:45:08,236 configuration_utils.py:301 INFO]: Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}
[2022-07-31 07:45:08,325 modeling_utils.py:650 INFO]: loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/v-zuangao/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
[2022-07-31 07:45:12,213 main.py:223 INFO]: Classification Acc: 0.8565, F1: 0.7850, Precision: 0.9166, Recall: 0.7468, AUC-ROC: 0.9604
[2022-07-31 07:45:12,219 main.py:225 INFO]: Classification report:
              precision    recall  f1-score   support
           0       0.83      1.00      0.91       989
           1       1.00      0.49      0.66       391
    accuracy                           0.86      1380
   macro avg       0.92      0.75      0.78      1380
weighted avg       0.88      0.86      0.84      1380
[2022-07-31 07:45:12,220 main.py:227 INFO]: Classification confusion matrix:
[[989   0]
 [198 193]]